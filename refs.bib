
@book{Baumer2017,
	title={Modern Data Science with R},
	year={2017},
	author={Baumer, Benjamin S.and Kaplan, Daniel T. and Horton, Nicholas J.},
	publisher={CRC Press},
    address={Boca Raton, FL},
	doi={10.1201/9781315113760},
	url={http://dx.doi.org/10.1201/9781315113760}
}

 @misc{Arvai2022,
    title={K-means clustering in Python: A practical guide},
    url={https://realpython.com/k-means-clustering-python/#how-to-perform-k-means-clustering-in-python},
    journal={Real Python},
    publisher={Real Python},
    author={Arvai, Kevin},
    year={2022},
    month={Sep}
}

@misc{KDChauhan2022,
   title={Decision Tree Algorithm, Explained},
   url={https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html},
   journal={KDnuggets},
   publisher={KDnuggets},
   author={Chauhan, Nagesh Singh},
   year={2022},
   month={Feb}
 }

@book{Hastie2009,
   title={The elements of statistical learning: data mining, inference, and prediction},
   year={2009},
   author={Hastie,Trevor and Tibshirani,Robert and Friedman,Jerome H.},
   publisher={Springer},
   address={New York, NY},
   edition={2.},
   keywords={Bioinformatics; Computational intelligence; Data mining; Datenanalyse; Forecasting; Inference; Lernen; Machine learning; Maschinelles Lernen; Methodology; Statistics; Statistik},
   isbn={0387848576;9780387848570;},
   language={English}
 }

 @article{Neth2017,
    title={FFTrees: A toolbox to create, visualize, and evaluate fast-and-frugal decision trees},
    author={Phillips,Nathaniel D. and Neth,Hansjoerg and Woike,Jan K. and Gaissmaier,Wolfgang},
    year={2017},
    journal={Judgment and decision making},
    volume={12},
    number={4},
    pages={344-368},
    abstract={Fast-and-frugal trees (FFTs) are simple algorithms that facilitate efficient and accurate decisions based on limited information. But despite their successful use in many applied domains, there is no widely available toolbox that allows anyone to easily create, visualize, and evaluate FFTs. We fill this gap by introducing the R package FFTrees. In this paper, we explain how FFTs work, introduce a new class of algorithms called fan for constructing FFTs, and provide a tutorial for using the FFTrees package. We then conduct a simulation across ten real-world datasets to test how well FFTs created by FFTrees can predict data. Simulation results show that FFTs created by FFTrees can predict data as well as popular classification algorithms such as regression and random forests, while remaining simple enough for anyone to understand and use.;Fast-and-frugal trees (FFTs) are simple algorithms that facilitate efficient and accurate decisions based on limited information. But despite their successful use in many applied domains, there is no widely available toolbox that allows anyone to easily create, visualize, and evaluate FFTs. We fill this gap by introducing the R package FFTrees. In this paper, we explain how FFTs work, introduce a new class of algorithms called fan for constructing FFTs, and provide a tutorial for using the FFTrees package. We then conduct a simulation across ten real-world datasets to test how well FFTs created by FFTrees can predict data. Simulation results show that FFTs created by FFTrees can predict data as well as popular classification algorithms such as regression and random forests, while remaining simple enough for anyone to understand and use. Keywords: decision trees, heuristics, prediction.},
    isbn={1930-2975},
    language={English}
 }

 @misc{ibm,
    title={What is a decision tree},
    url={https://www.ibm.com/topics/decision-trees},
    journal={IBM},
    publisher={IBM},
    urldate={2022-11-30}
}

@book{Hunt1966,
   author={Hunt,Earl B. and Marin,Janet and Stone,Philip J.},
   year={1966},
   title={Experiments in induction},
   publisher={Academic Pr},
   address={New York},
   keywords={Begriffsbildung; Computersimulation; Lernen; Problemlösen},
   language={English}
}

@article{Quinlan1986,
   title={Induction of decision trees},
   author={Quinlan, John Ross},
   year={1986},
   journal={Machine Learning},
   volume={1},
   number={1},
   pages={81-106},
   isbn={1573-0565},
   doi={10.1007/BF00116251},
   url={https://doi.org/10.1007/BF00116251},
   language={English},
   abstract={The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.}
}

@book{Quinlan2003,
   author={Quinlan, John Ross},
   year={2003},
   title={C4.5: programs for machine learning},
   publisher={Morgan Kaufmann},
   ddress={Amsterdam;Heidelberg;},
   edition={5. [pr.]},
   keywords={Algorithms; Algorithmus; C4.5; Computer programming; Machine learning; Maschinelles Lernen; Programmierung},
   isbn={9781558602380;1558602380;},
   language={English}
}

@book{Breiman1984,
   author={Breiman,Leo},
   year={1984},
   title={Classification and regression trees},
   publisher={Wadsworth Internat. Group},
   address={Belmont, Calif},
   keywords={Baum; Klassifikationstheorie; Regressionsanalyse},
   sbn={9780534980535;0534980546;9780534980542;0534980538;},
   language={English}
}

@article{deVille2013,
   author={de Ville, Barry},
   itle={Decision trees},
   year={2013},
   journal={WIREs Computational Statistics},
   volume={5},
   number={6},
   pages={448-455},
   keywords={decision trees, rule induction, predictive models, machine learning, boosting, random forests},
   doi={https://doi.org/10.1002/wics.1278},
   url={https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1278},
   abstract={Decision trees trace their origins to the era of the early development of written records. This history illustrates a major strength of trees: exceptionally interpretable results which have an intuitive tree-like display which, in turn, enhances understanding and the dissemination of results. The computational origins of decision trees—sometimes called classification trees or regression trees—are models of biological and cognitive processes. This common heritage drives complementary developments of both statistical decision trees and trees designed for machine learning. The unfolding and progressive elucidation of the various features of trees throughout their early history in the late 20th century is discussed along with the important associated reference points and responsible authors. Statistical approaches, such as a hypothesis testing and various resampling approaches, have coevolved along with machine learning implementations. This had resulted in exceptionally adaptable decision tree tools, appropriate for various statistical and machine learning tasks, across various levels of measurement, with varying levels of data quality. Trees are robust in the presence of missing data and offer multiple ways of incorporating missing data in the resulting models. Although trees are powerful, they are also flexible and easy to use methods. This assures the production of high quality results that require few assumptions to deploy. The treatment ends with a discussion of the most current developments which continue to rely on the synergies and cross-fertilization between statistical and machine learning communities. Current developments with the emergence of multiple trees and the various resampling approaches that are employed are discussed. WIREs Comput Stat 2013, 5:448–455. doi: 10.1002/wics.1278 This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences > Clustering and Classification Statistical Learning and Exploratory Methods of the Data Sciences > Pattern Recognition Statistical Learning and Exploratory Methods of the Data Sciences > Rule-Based Mining}
}

@misc{scikitDT,
   title={Decision Trees},
   url={https://scikit-learn.org/stable/modules/tree.html},
   journal={scikit-learn},
   publisher={scikit-learn},
   urldate={2022-11-30}
}
