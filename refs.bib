
@book{Baumer2017,
	title={Modern Data Science with R},
	year={2017},
	author={Baumer, Benjamin S.and Kaplan, Daniel T. and Horton, Nicholas J.},
	publisher={CRC Press},
    address={Boca Raton, FL},
	doi={10.1201/9781315113760},
	url={http://dx.doi.org/10.1201/9781315113760}
}

 @misc{Arvai2022,
    title={K-means clustering in Python: A practical guide},
    url={https://realpython.com/k-means-clustering-python/#how-to-perform-k-means-clustering-in-python},
    journal={Real Python},
    publisher={Real Python},
    author={Arvai, Kevin},
    year={2022},
    month={Sep}
}

@misc{KDChauhan2022,
   title={Decision Tree Algorithm, Explained},
   url={https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html},
   journal={KDnuggets},
   publisher={KDnuggets},
   author={Chauhan, Nagesh Singh},
   year={2022},
   month={Feb}
 }

@book{Hastie2009,
   title={The elements of statistical learning: data mining, inference, and prediction},
   year={2009},
   author={Hastie,Trevor and Tibshirani,Robert and Friedman,Jerome H.},
   publisher={Springer},
   address={New York, NY},
   edition={2.},
   keywords={Bioinformatics; Computational intelligence; Data mining; Datenanalyse; Forecasting; Inference; Lernen; Machine learning; Maschinelles Lernen; Methodology; Statistics; Statistik},
   isbn={0387848576;9780387848570;},
   language={English}
 }

 @article{Neth2017,
    title={FFTrees: A toolbox to create, visualize, and evaluate fast-and-frugal decision trees},
    author={Phillips,Nathaniel D. and Neth,Hansjoerg and Woike,Jan K. and Gaissmaier,Wolfgang},
    year={2017},
    journal={Judgment and decision making},
    volume={12},
    number={4},
    pages={344-368},
    abstract={Fast-and-frugal trees (FFTs) are simple algorithms that facilitate efficient and accurate decisions based on limited information. But despite their successful use in many applied domains, there is no widely available toolbox that allows anyone to easily create, visualize, and evaluate FFTs. We fill this gap by introducing the R package FFTrees. In this paper, we explain how FFTs work, introduce a new class of algorithms called fan for constructing FFTs, and provide a tutorial for using the FFTrees package. We then conduct a simulation across ten real-world datasets to test how well FFTs created by FFTrees can predict data. Simulation results show that FFTs created by FFTrees can predict data as well as popular classification algorithms such as regression and random forests, while remaining simple enough for anyone to understand and use.;Fast-and-frugal trees (FFTs) are simple algorithms that facilitate efficient and accurate decisions based on limited information. But despite their successful use in many applied domains, there is no widely available toolbox that allows anyone to easily create, visualize, and evaluate FFTs. We fill this gap by introducing the R package FFTrees. In this paper, we explain how FFTs work, introduce a new class of algorithms called fan for constructing FFTs, and provide a tutorial for using the FFTrees package. We then conduct a simulation across ten real-world datasets to test how well FFTs created by FFTrees can predict data. Simulation results show that FFTs created by FFTrees can predict data as well as popular classification algorithms such as regression and random forests, while remaining simple enough for anyone to understand and use. Keywords: decision trees, heuristics, prediction.},
    isbn={1930-2975},
    language={English}
 }

 @misc{ibm,
    title={What is a decision tree},
    url={https://www.ibm.com/topics/decision-trees},
    journal={IBM},
    publisher={IBM},
    urldate={2022-11-30}
}

@book{Hunt1966,
   author={Hunt,Earl B. and Marin,Janet and Stone,Philip J.},
   year={1966},
   title={Experiments in induction},
   publisher={Academic Pr},
   address={New York},
   keywords={Begriffsbildung; Computersimulation; Lernen; Problemlösen},
   language={English}
}

@article{Quinlan1986,
   title={Induction of decision trees},
   author={Quinlan, John Ross},
   year={1986},
   journal={Machine Learning},
   volume={1},
   number={1},
   pages={81-106},
   isbn={1573-0565},
   doi={10.1007/BF00116251},
   url={https://doi.org/10.1007/BF00116251},
   language={English},
   abstract={The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.}
}

@book{Quinlan2003,
   author={Quinlan, John Ross},
   year={2003},
   title={C4.5: programs for machine learning},
   publisher={Morgan Kaufmann},
   ddress={Amsterdam;Heidelberg;},
   edition={5. [pr.]},
   keywords={Algorithms; Algorithmus; C4.5; Computer programming; Machine learning; Maschinelles Lernen; Programmierung},
   isbn={9781558602380;1558602380;},
   language={English}
}

@book{Breiman1984,
   author={Breiman,Leo},
   year={1984},
   title={Classification and regression trees},
   publisher={Wadsworth Internat. Group},
   address={Belmont, Calif},
   keywords={Baum; Klassifikationstheorie; Regressionsanalyse},
   isbn={9780534980535;0534980546;9780534980542;0534980538;},
   language={English}
}

@article{deVille2013,
   author={de Ville, Barry},
   itle={Decision trees},
   year={2013},
   journal={WIREs Computational Statistics},
   volume={5},
   number={6},
   pages={448-455},
   keywords={decision trees, rule induction, predictive models, machine learning, boosting, random forests},
   doi={https://doi.org/10.1002/wics.1278},
   url={https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1278},
   abstract={Decision trees trace their origins to the era of the early development of written records. This history illustrates a major strength of trees: exceptionally interpretable results which have an intuitive tree-like display which, in turn, enhances understanding and the dissemination of results. The computational origins of decision trees—sometimes called classification trees or regression trees—are models of biological and cognitive processes. This common heritage drives complementary developments of both statistical decision trees and trees designed for machine learning. The unfolding and progressive elucidation of the various features of trees throughout their early history in the late 20th century is discussed along with the important associated reference points and responsible authors. Statistical approaches, such as a hypothesis testing and various resampling approaches, have coevolved along with machine learning implementations. This had resulted in exceptionally adaptable decision tree tools, appropriate for various statistical and machine learning tasks, across various levels of measurement, with varying levels of data quality. Trees are robust in the presence of missing data and offer multiple ways of incorporating missing data in the resulting models. Although trees are powerful, they are also flexible and easy to use methods. This assures the production of high quality results that require few assumptions to deploy. The treatment ends with a discussion of the most current developments which continue to rely on the synergies and cross-fertilization between statistical and machine learning communities. Current developments with the emergence of multiple trees and the various resampling approaches that are employed are discussed. WIREs Comput Stat 2013, 5:448–455. doi: 10.1002/wics.1278 This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences > Clustering and Classification Statistical Learning and Exploratory Methods of the Data Sciences > Pattern Recognition Statistical Learning and Exploratory Methods of the Data Sciences > Rule-Based Mining}
}

@misc{scikitDT,
   title={Decision Trees},
   url={https://scikit-learn.org/stable/modules/tree.html},
   journal={scikit-learn},
   publisher={scikit-learn},
   urldate={2022-11-30}
}

@article{Winter2021,
    author = {Winter, Bodo and Bürkner, Paul-Christian},
    title = {Poisson regression for linguists: A tutorial introduction to modelling count data with brms},
    journal = {Language and Linguistics Compass},
    volume = {15},
    number = {11},
    pages = {e12439},
    doi = {https://doi.org/10.1111/lnc3.12439},
    url = {https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12439},
    eprint = {https://compass.onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12439},
    abstract = {Abstract Count data is prevalent in many different areas of linguistics, such as when counting words, syntactic constructions, discourse particles, case markers, or speech errors. The Poisson distribution is the canonical distribution for characterising count data with no or unknown upper bound. Given the prevalence of count data in linguistics, Poisson regression has wide utility no matter what subfield of linguistics is considered. However, in contrast to logistic regression, Poisson regression is surprisingly little known. Here, we make a case for why linguists need to consider Poisson regression, and give recommendations for when Poisson regression is more appropriate compared to logistic regression. This tutorial introduces readers to foundational concepts needed to understand the basics of Poisson regression, followed by a hands-on tutorial using the R package brms. We discuss a dataset where Catalan and Korean speakers change the frequency of their co-speech gestures as a function of politeness contexts. This dataset also involves exposure variables (the incorporation of time to deal with unequal intervals) and overdispersion (excess variance). Altogether, we hope that more linguists will consider Poisson regression for the analysis of count data.},
    year = {2021}
}

@misc{MarkovChains_Ritvik2020,
    title={Markov Chains : Data Science Basics},
    url={https://www.youtube.com/watch?v=prZMpThbU3E},
    journal={YouTube},
    author={ritvikmath},
    year={2020},
    date={2020-09-09},
    urldate={2023-25-03},
    type={YouTube Video}
}

@misc{MonteCarlo_Ritvik2021,
    title={Monte Carlo Methods : Data Science Basics},
    url={https://www.youtube.com/watch?v=EaR3C4e600k},
    journal={YouTube},
    author={ritvikmath},
    year={2021},
    date={2021-06-01},
    urldate={2023-25-03},
    type={YouTube Video}
}

@misc{MCMC_Ritvik2021,
    title={Markov Chain Monte Carlo (MCMC) : Data Science Concepts},
    url={https://www.youtube.com/watch?v=yApmR-c_hKU},
    journal={YouTube},
    author={ritvikmath},
    year={2021},
    date={2021-20-01},
    urldate={2023-25-03},
    type={YouTube Video}
}

@article{SD32014,
    author = {Jones, Daniel N. and Paulhus, Delroy L.},
    title ={Introducing the Short Dark Triad (SD3): A Brief Measure of Dark Personality Traits},
    journal = {Assessment},
    volume = {21},
    number = {1},
    pages = {28-41},
    year = {2014},
    doi = {10.1177/1073191113514105},
    note ={PMID: 24322012},
    URL = {https://doi.org/10.1177/1073191113514105},
    eprint = {https://doi.org/10.1177/1073191113514105},
    abstract = { Three socially aversive traits—Machiavellianism, narcissism, and psychopathy—have been studied as an overlapping constellation known as the Dark Triad. Here, we develop and validate the Short Dark Triad (SD3), a brief proxy measure. Four studies (total N = 1,063) examined the structure, reliability, and validity of the subscales in both community and student samples. In Studies 1 and 2, structural analyses yielded three factors with the final 27 items loading appropriately on their respective factors. Study 3 confirmed that the resulting SD3 subscales map well onto the longer standard measures. Study 4 validated the SD3 subscales against informant ratings. Together, these studies indicate that the SD3 provides efficient, reliable, and valid measures of the Dark Triad of personalities. }
}

@misc{Vehtari2022,
   title={Bayesian Logistic Regression with rstanarm},
   url={https://avehtari.github.io/modelselection/diabetes.html},
   publisher={Aki Vehtari: Model Selection},
   author={Vehtari, Aki and Gabry, Jonah and Goodrich, Ben},
   year={2022},
   month={Feb},
   type = {Web Page}
}

@article{Etz2018,
   author = {Etz, Alexander and Vandekerckhove, Joachim},
   title = {Introduction to Bayesian Inference for Psychology},
   journal = {Psychonomic bulletin & review},
   volume = {25},
   number = {1},
   pages = {5-34},
   ISSN = {1069-9384},
   DOI = {10.3758/s13423-017-1262-3},
   url = {https://go.exlibris.link/Y9t8PThD},
   year = {2018},
   type = {Journal Article}
}

@inbook{Fishburn1988,
   author = {Fishburn, P. C.},
   title = {Normative Theories of Decision Making Under Risk and Under Uncertainty},
   booktitle = {Decision Making: Descriptive, Normative, and Prescriptive Interactions},
   editor = {Tversky, Amos and Bell, David E. and Raiffa, Howard},
   publisher = {Cambridge University Press},
   address = {Cambridge},
   pages = {78-98},
   ISBN = {9780521368513},
   DOI = {10.1017/CBO9780511598951.006},
   url = {https://www.cambridge.org/core/books/decision-making/normative-theories-of-decision-making-under-risk-and-under-uncertainty/6FFE429FE18AA1013DF96E0BE77D508B},
   year = {1988},
   type = {Book Section}
}

@book{Robert2007,
   author = {Robert, Christian P.},
   title = {The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation},
   publisher = {Springer},
   address = {New York, NY},
   edition = {2},
   series = {Springer Texts in Statistics},
   pages = {615},
   ISBN = {978-0-387-95231-4; 978-0-387-71598-8; 978-0-387-71599-5},
   DOI = {10.1007/0-387-71599-1},
   year = {2007},
   type = {Book}
}

@book{Johnson2022,
   author = {Johnson, Alicia A. and Ott, Miles Q. and Dogucu, Mine},
   title = {Bayes rules: an introduction to Bayesian modeling},
   publisher = {CRC Press Taylor & Francis Group},
   address = {New York;Boca Raton;London;},
   ISBN = {9781032191591;1032191597;0367255391;9780367255398;},
   url = {https://go.exlibris.link/bsmRK8wq},
   year = {2022},
   type = {Book}
}

@book{Clyde2022,
   author = {Clyde, Merlise and Çetinkaya-Rundel, Mine and Rundel, Colin and Banks, David and Chai, Christine and Huang, Lizzy},
   title = {An Introduction to Bayesian Thinking},
   url = {https://statswithr.github.io/book/},
   year = {2022},
   type = {Web Book}
}

@book{Bozza2022,
   author = {Bozza, Silvia and Taroni, Franco and Biedermann, Alex},
   title = {Bayes Factors for Forensic Decision Analyses with R},
   publisher = {Springer International Publishing},
   address = {Cham},
   edition = {1st 2022.},
   ISBN = {9783031098390;3031098390;3031098382;9783031098383;},
   DOI = {10.1007/978-3-031-09839-0},
   url = {https://go.exlibris.link/HFPWLKtR},
   year = {2022},
   type = {Book}
}

@Book{Cookbook,
  title = {R Markdown Cookbook},
  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2022},
  isbn = {9780367563837},
  url = {https://bookdown.org/yihui/rmarkdown-cookbook},
}
