[["index.html", "Advanced Data Analysis in R and Python Preface Contact the Author", " Advanced Data Analysis in R and Python Hamilkar Constantin Oueslati 2022-11-30 Preface This book shall document my efforts in learning different methods used in the analysis of psychological data. Since this book was created at the beginning of my master’s programme in psychology at the University of Constance (winter of 2022), this repository will only include methods, that were taught to me (by myself or others) during the course of said master’s programme. Contact the Author In case you wish to contact me, please use the following contact details: Hamilkar Constantin Oueslati Pronouns: they/them (engl.) and dey/deren/denen (ger.) Undergraduate Student at the University of Konstanz Study Programme: Psychology (M.Sc.) Mail: hamilkar-constantin.oueslati@uni-konstanz.de Web: https://hco-consulting.eu "],["prerequisites.html", "Chapter 1 Prerequisites 1.1 Load R Packages 1.2 Load Python Version 3.10.8 1.3 Import Python Modules", " Chapter 1 Prerequisites 1.1 Load R Packages library(dplyr) library(tidyr) library(ggplot2) library(mclust) library(psych) library(gridExtra) library(plotly) library(processx) library(reticulate) library(DiagrammeR) library(performance) library(afex) library(qqplotr) library(bookdown) library(FFTrees) library(rpart) library(rpart.plot) 1.2 Load Python Version 3.10.8 use_python(&quot;/Library/Frameworks/Python.framework/Versions/3.10/bin/python3&quot;) 1.3 Import Python Modules import numpy as np import matplotlib.pyplot as plt import pandas as pd import statsmodels.api as sm import statsmodels.formula.api as smf import statsmodels.stats.api as sms import statsmodels.stats.descriptivestats as smds from kneed import KneeLocator from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score from sklearn.preprocessing import StandardScaler from sklearn import tree from sklearn.datasets import load_iris import plotly.express as px import plotly.io as pio import plotly.graph_objects as go from plotly.subplots import make_subplots import kaleido import psutil "],["brushing-up-on-the-basics.html", "Chapter 2 Brushing Up on the Basics 2.1 Mixed ANOVA R", " Chapter 2 Brushing Up on the Basics 2.1 Mixed ANOVA R Below I have conducted a mixed ANOVA based on the data from the dataset iris. The dependent variable used in said calculation are the respective measurements recorded in the dataset. “Species” is the between-factor, with “Setosa”, “Versicolor” and “Virginica” being its factor levels. “Part of Flower” is the first within-factor with “Petal” and “Sepal” being its factor levels. “Kind of Measurement” is the second within-factor with “Width” and “Length” being its factor levels. 2.1.1 Preperation of Dataset iris R Before a mixed ANOVA analysis can be conducted the data in the dataset iris needs to be reordered and recoded accordingly. # Create custom function to recode variables recode.var &lt;- function(x, # what vector do you want to recode? old, # what values do you want to change? new, # what should the new values be? otherNA = TRUE, # should other values be converted to NA? numeric = TRUE) { # should result be numeric? x.new &lt;- x # copy vector to x.new if(class(x.new) == &quot;factor&quot;) {x.new &lt;- paste(x.new)} # remove factors for(i in 1:length(old)) { # loop through all old values: x.new[x == old[i]] &lt;- new[i] } if(otherNA) { # convert unspecified values to NA: x.new[(x %in% old) == FALSE] &lt;- NA } if(numeric) {x.new &lt;- as.numeric(x.new)} # convert vector to numeric values return(x.new) # return new vector } # Create appropriate subset of &#39;iris&#39; iris_anova_prep1 &lt;- iris[,1:4] iris_anova_prep1$id &lt;- c(1:150) # Rename and recode &#39;TCategory&#39; for better readability iris_anova_prep1$Species &lt;- recode.var(iris$TCategory, old = c(&quot;I.setosa&quot;, &quot;I.versicolor&quot;, &quot;I.virginica&quot;), new = c(&quot;Setosa&quot;, &quot;Versicolor&quot;, &quot;Virginica&quot;), otherNA = FALSE, numeric = FALSE) # Save &#39;Species&#39; as factor variable iris_anova_prep1$Species &lt;- as.factor(iris_anova_prep1$Species) # Reformat dataframe from wide to long format with temporary variable &#39;wtifl_prep&#39; iris_anova_prep1 &lt;- gather(iris_anova_prep1, wtifl_prep, Measurement, Sepal_length:Petal_width, factor_key=TRUE) head(iris_anova_prep1) ## id Species wtifl_prep Measurement ## 1 1 Setosa Sepal_length 5.1 ## 2 2 Setosa Sepal_length 4.9 ## 3 3 Setosa Sepal_length 4.7 ## 4 4 Setosa Sepal_length 4.6 ## 5 5 Setosa Sepal_length 5.0 ## 6 6 Setosa Sepal_length 5.4 # Prepare creation of within-factor variables &#39;Part_of_Flower&#39; and &#39;Kind_of_Measurement&#39; iris_anova_prep2 &lt;- iris_anova_prep1 iris_anova_prep2$Part_of_Flower &lt;- iris_anova_prep2$wtifl_prep iris_anova_prep2$Kind_of_Measurement &lt;- iris_anova_prep2$wtifl_prep # Split temporary variable &#39;wtifl_prep&#39; into within-factor variables &#39;Part_of_Flower&#39; and &#39;Kind_of_Measurement&#39; iris_anova_prep2$Part_of_Flower &lt;- recode.var(iris_anova_prep2$Part_of_Flower, old = c(&quot;Sepal_length&quot;, &quot;Sepal_width&quot;, &quot;Petal_length&quot;, &quot;Petal_width&quot;), new = c(&quot;Sepal&quot;, &quot;Sepal&quot;, &quot;Petal&quot;, &quot;Petal&quot;), otherNA = FALSE, numeric = FALSE) iris_anova_prep2$Kind_of_Measurement &lt;- recode.var(iris_anova_prep2$Kind_of_Measurement, old = c(&quot;Sepal_length&quot;, &quot;Sepal_width&quot;, &quot;Petal_length&quot;, &quot;Petal_width&quot;), new = c(&quot;Length&quot;, &quot;Width&quot;, &quot;Length&quot;, &quot;Width&quot;), otherNA = FALSE, numeric = FALSE) # Save &#39;Part_of_Flower&#39; and &#39;Kind_of_Measurement&#39; as factor variables iris_anova_prep2$Part_of_Flower &lt;- as.factor(iris_anova_prep2$Part_of_Flower) iris_anova_prep2$Kind_of_Measurement &lt;- as.factor(iris_anova_prep2$Kind_of_Measurement) # Delete temporary variable &#39;wtifl_prep&#39; iris_anova &lt;- iris_anova_prep2[,c(1:2,5,6,4)] head(iris_anova) ## id Species Part_of_Flower Kind_of_Measurement Measurement ## 1 1 Setosa Sepal Length 5.1 ## 2 2 Setosa Sepal Length 4.9 ## 3 3 Setosa Sepal Length 4.7 ## 4 4 Setosa Sepal Length 4.6 ## 5 5 Setosa Sepal Length 5.0 ## 6 6 Setosa Sepal Length 5.4 2.1.2 Creating the ANOVA Model iris_anova_model &lt;- aov_ez(&quot;id&quot;, &quot;Measurement&quot;, iris_anova, between = &quot;Species&quot;, within = c(&quot;Part_of_Flower&quot;, &quot;Kind_of_Measurement&quot;)) Note. Type III Sums of Squares are used for the calculation of the ANOVA 2.1.3 Testing Assumptions of the ANOVA R 2.1.3.1 Observations are Independent and Identically Distributed This assumption cannot be tested empirically, but needs to hold on conceptual or logical grounds. For the purpose of this exercise I am therefore simply assuming, that said assumption holds. 2.1.3.2 Homogeneity of Variances The variances across all the groups (cells) of between-subject effects should be the same. In order to test this assumption Levene’s Test is conducted. check_homogeneity(iris_anova_model) ## Warning: Variances differ between groups (Levene&#39;s Test, p = 0.001). These results indicate that the assumption of homogeneity of variances is in fact significantly violated. 2.1.3.3 Sphericity For within-subjects effects, sphericity is the condition where the variances of the differences between all possible pairs of within-subject conditions (i.e., levels of the independent variable) are equal. In order to test this assumption Mauchly’s test is conducted. check_sphericity(iris_anova_model) ## OK: Data seems to be spherical (p &gt; .999). These results indicate that the assumption of sphericity is not significantly violated. 2.1.3.4 Normality of Residuals The errors used for the estimation of the error term(s) (MSE) are normally distributed. In order to test this assumption the Shapiro-Wilk test is conducted. iris_anova_norm &lt;- check_normality(iris_anova_model) iris_anova_norm ## Warning: Non-normality of residuals detected (p &lt; .001). Since this test tends to have high type-I error rates, a visual inspection of the residuals using a quantile-quantile plot (qq-plots) is preferred. plot(iris_anova_norm, type = &quot;qq&quot;) Figure 2.1: Quantile-Quantile plot If the residuals were normally distributed, we would see them falling close to the diagonal line, inside the 95% confidence bands around the qq-line. Unfortunately it seems, that the assumption of the normality of residuals is in deed significantly violated. 2.1.3.5 Conclusion Since this is just an exercise in conducting ANOVA analyses, the above results are simply ignored and the analysis was conducted nonetheless. 2.1.4 Results of Mixed ANOVA summary(iris_anova_model) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value ## (Intercept) 7198.2 1 53.966 147 19607.2785 ## Species 310.5 2 53.966 147 422.9379 ## Part_of_Flower 582.1 1 10.664 147 8024.8551 ## Species:Part_of_Flower 152.6 2 10.664 147 1051.5113 ## Kind_of_Measurement 1073.1 1 20.254 147 7788.1235 ## Species:Kind_of_Measurement 127.9 2 20.254 147 463.9684 ## Part_of_Flower:Kind_of_Measurement 2.0 1 4.503 147 64.3902 ## Species:Part_of_Flower:Kind_of_Measurement 0.5 2 4.503 147 8.0009 ## Pr(&gt;F) ## (Intercept) &lt; 2.2e-16 *** ## Species &lt; 2.2e-16 *** ## Part_of_Flower &lt; 2.2e-16 *** ## Species:Part_of_Flower &lt; 2.2e-16 *** ## Kind_of_Measurement &lt; 2.2e-16 *** ## Species:Kind_of_Measurement &lt; 2.2e-16 *** ## Part_of_Flower:Kind_of_Measurement 2.974e-13 *** ## Species:Part_of_Flower:Kind_of_Measurement 0.0005031 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["unsupervised-learning.html", "Chapter 3 Unsupervised Learning 3.1 Non-Hierarchical Clustering R 3.2 Non-Hierarchical Clustering Python", " Chapter 3 Unsupervised Learning 3.1 Non-Hierarchical Clustering R 3.1.1 Import Dataset iris R iris &lt;- read.csv(&quot;Data/iris.dat&quot;, row.names=NULL) iris$TCategory &lt;- as.factor(iris$TCategory) iris$Category &lt;- as.factor(iris$Category) # Inspecting the imported dataset glimpse(iris) ## Rows: 150 ## Columns: 6 ## $ Sepal_length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8… ## $ Sepal_width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0… ## $ Petal_length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4… ## $ Petal_width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1… ## $ Category &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ TCategory &lt;fct&gt; I.setosa, I.setosa, I.setosa, I.setosa, I.setosa, I.setosa, I.s… str(iris) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ Sepal_length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal_width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal_length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal_width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Category : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ TCategory : Factor w/ 3 levels &quot;I.setosa&quot;,&quot;I.versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... describe(iris) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## Sepal_length 1 150 5.84 0.83 5.80 5.81 1.04 4.3 7.9 3.6 0.31 -0.61 0.07 ## Sepal_width 2 150 3.05 0.43 3.00 3.04 0.37 2.0 4.4 2.4 0.33 0.20 0.04 ## Petal_length 3 150 3.76 1.76 4.35 3.76 1.85 1.0 6.9 5.9 -0.27 -1.42 0.14 ## Petal_width 4 150 1.20 0.76 1.30 1.18 1.04 0.1 2.5 2.4 -0.10 -1.36 0.06 ## Category* 5 150 2.00 0.82 2.00 2.00 1.48 1.0 3.0 2.0 0.00 -1.52 0.07 ## TCategory* 6 150 2.00 0.82 2.00 2.00 1.48 1.0 3.0 2.0 0.00 -1.52 0.07 3.1.2 Clustering Using a k-means Algorithm Using the R package mclust I have chosen to use a k-means clustering algorithm to separate the flowers into three clusters based on their sepal length and width as well as on their petal length and width. #|label: clustering_k-means_r_0 #| tidy: true #| tidy-opts: #| - width.cutoff=60 iris &lt;- read.csv(&quot;Data/iris.dat&quot;, row.names=NULL) iris_wo_cat &lt;- iris[,1:4] set.seed(15) iris_clusters &lt;- iris_wo_cat %&gt;% kmeans(centers = 3) %&gt;% fitted(&quot;classes&quot;) %&gt;% as.character() iris &lt;- iris %&gt;% mutate(cluster = iris_clusters) This method was adapted from the method described in chapter 9.1.2 in the book Modern Data Science with R by Baumer et al. (2017). 3.1.3 Visualization of Results of Clustering R #|label: clustering_k-means_r_1 #| tidy: true #| tidy-opts: #| - width.cutoff=60 iris_c_plot_1 &lt;- plot_ly( data = iris, x = ~Category, y = ~Sepal_length, color = ~cluster, type = &quot;scatter&quot;, mode = &quot;markers&quot;) iris_c_plot_1 &lt;- iris_c_plot_1 %&gt;% layout(xaxis = list(title = &quot;Species&quot;, showticklabels = TRUE, ticktext = list(&quot;Iris Setosa&quot;, &quot;Iris Versicolor&quot;, &quot;Iris Virginica&quot;), tickvals = list(0, 1, 2), tickmode = &quot;array&quot;, zeroline = FALSE), yaxis = list(title = &quot;Sepal Length [cm]&quot;, showticklabels = TRUE, zeroline = FALSE), legend = list(title = list(text=&#39;&lt;b&gt; Cluster &lt;/b&gt;&#39;))) iris_c_plot_1 Figure 3.1: Categorization vs. Clustering - plotted against Sepal Length using *R* #|label: clustering_k-means_r_2 #| tidy: true #| tidy-opts: #| - width.cutoff=60 iris_c_plot_2 &lt;- plot_ly( data = iris, x = ~Category, y = ~Sepal_width, color = ~cluster, type = &quot;scatter&quot;, mode = &quot;markers&quot;, showlegend = FALSE) iris_c_plot_2 &lt;- iris_c_plot_2 %&gt;% layout(xaxis = list(title = &quot;Species&quot;, showticklabels = TRUE, ticktext = list(&quot;Iris Setosa&quot;, &quot;Iris Versicolor&quot;, &quot;Iris Virginica&quot;), tickvals = list(0, 1, 2), tickmode = &quot;array&quot;, zeroline = FALSE), yaxis = list(title = &quot;Sepal Width [cm]&quot;, showticklabels = TRUE, zeroline = FALSE)) iris_c_plot_2 Figure 3.2: Categorization vs. Clustering - plotted against Sepal Width using *R* #|label: clustering_k-means_r_3 #| tidy: true #| tidy-opts: #| - width.cutoff=60 iris_c_plot_3 &lt;- plot_ly( data = iris, x = ~Category, y = ~Petal_length, color = ~cluster, type = &quot;scatter&quot;, mode = &quot;markers&quot;, showlegend = FALSE) iris_c_plot_3 &lt;- iris_c_plot_3 %&gt;% layout(xaxis = list(title = &quot;Species&quot;, showticklabels = TRUE, ticktext = list(&quot;Iris Setosa&quot;, &quot;Iris Versicolor&quot;, &quot;Iris Virginica&quot;), tickvals = list(0, 1, 2), tickmode = &quot;array&quot;, zeroline = FALSE), yaxis = list(title = &quot;Petal Length [cm]&quot;, showticklabels = TRUE, zeroline = FALSE)) iris_c_plot_3 Figure 3.3: Categorization vs. Clustering - plotted against Petal Length using *R* #|label: clustering_k-means_r_4 #| tidy: true #| tidy-opts: #| - width.cutoff=60 iris_c_plot_4 &lt;- plot_ly( data = iris, x = ~Category, y = ~Petal_width, color = ~cluster, type = &quot;scatter&quot;, mode = &quot;markers&quot;, showlegend = FALSE) iris_c_plot_4 &lt;- iris_c_plot_4 %&gt;% layout(xaxis = list(title = &quot;Species&quot;, showticklabels = TRUE, ticktext = list(&quot;Iris Setosa&quot;, &quot;Iris Versicolor&quot;, &quot;Iris Virginica&quot;), tickvals = list(0, 1, 2), tickmode = &quot;array&quot;, zeroline = FALSE), yaxis = list(title = &quot;Petal Width [cm]&quot;, showticklabels = TRUE, zeroline = FALSE)) iris_c_plot_4 Figure 3.4: Categorization vs. Clustering - plotted against Petal Width using *R* 3.2 Non-Hierarchical Clustering Python 3.2.1 Import Dataset iris Python # Import dataset &#39;iris&#39; as a &#39;Pandas Dataframe&#39; iris_df= pd.read_csv(&#39;Data/iris.dat&#39;, header=0) # Import dataset &#39;iris&#39; as a &#39;NumPy Array&#39; iris_arr= pd.read_csv(&#39;Data/iris.dat&#39;, header=0).values # Defining a function that checks whether the dataset is a &#39;Pandas Dataframe&#39; # or a &#39;NumPy Array&#39; and prints the structure of said dataset. def data_structure(dataset): if isinstance(dataset, pd.DataFrame): print(&quot;The dataset is a &#39;Pandas Dataframe&#39;&quot;, &quot;\\n&quot;, &quot;Number of dimensions of dataframe: &quot;, dataset.ndim, &quot;\\n&quot;, &quot;Shape of dataset: &quot;, dataset.shape, &quot;\\n&quot;, &quot;Size of dataset: &quot;, dataset.size ) elif isinstance(dataset, np.ndarray): print(&quot;The dataset is a &#39;NumPy Array&#39;&quot;, &quot;\\n&quot;, &quot;Number of dimensions of dataframe: &quot;,dataset.ndim, &quot;\\n&quot;, &quot;Shape of dataset: &quot;, dataset.shape, &quot;\\n&quot;, &quot;Size of dataset: &quot;, dataset.size ) else: raise ValueError(&quot;Please, choose either a &#39;Pandas Dataframe&#39; or a &#39;NumPy Array&#39;.&quot;) # Inspecting the imported datasets print(&quot;Inspecting type and structure of &#39;iris_df&#39;&quot;) ## Inspecting type and structure of &#39;iris_df&#39; data_structure(iris_df) ## The dataset is a &#39;Pandas Dataframe&#39; ## Number of dimensions of dataframe: 2 ## Shape of dataset: (150, 6) ## Size of dataset: 900 print(&quot;Inspecting type and structure of &#39;iris_arr&#39;&quot;) ## Inspecting type and structure of &#39;iris_arr&#39; data_structure(iris_arr) ## The dataset is a &#39;NumPy Array&#39; ## Number of dimensions of dataframe: 2 ## Shape of dataset: (150, 6) ## Size of dataset: 900 iris_df.head ## &lt;bound method NDFrame.head of Sepal_length Sepal_width ... Category TCategory ## 0 5.1 3.5 ... 0 I.setosa ## 1 4.9 3.0 ... 0 I.setosa ## 2 4.7 3.2 ... 0 I.setosa ## 3 4.6 3.1 ... 0 I.setosa ## 4 5.0 3.6 ... 0 I.setosa ## .. ... ... ... ... ... ## 145 6.7 3.0 ... 2 I.virginica ## 146 6.3 2.5 ... 2 I.virginica ## 147 6.5 3.0 ... 2 I.virginica ## 148 6.2 3.4 ... 2 I.virginica ## 149 5.9 3.0 ... 2 I.virginica ## ## [150 rows x 6 columns]&gt; 3.2.2 Preprocessing of Data All the numerical features, which the machine learning algorithm should consider in the process of clustering, were measured in the same unit (cm). Due to that fact it is not strictly necessary to conduct feature scaling during the preprocessing of the data. Nevertheless I chose to standardize the data, since I am trying to teach myself the best-practice approach to using machine learning algorithms. More information on preprocessing data can be found here. # Choosing the data used for the clustering iris_ml = iris_arr[:, :4].copy() # Standardization scaler = StandardScaler() iris_ml_std = scaler.fit_transform(iris_ml) # Inspecting the data used for clustering data_structure(iris_ml_std) ## The dataset is a &#39;NumPy Array&#39; ## Number of dimensions of dataframe: 2 ## Shape of dataset: (150, 4) ## Size of dataset: 600 iris_ml_std[:5, :] ## array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], ## [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], ## [-1.38535265, 0.33784833, -1.39813811, -1.31297673], ## [-1.50652052, 0.10644536, -1.2844067 , -1.31297673], ## [-1.02184904, 1.26346019, -1.3412724 , -1.31297673]]) 3.2.3 Clustering Using a k-means Algorithm In order to be able to compare workflows I searched for a clustering method in Python, which is similar to the one I adapted from Baumer et al. (2017). During my search I learned a great deal more about the k-means algorithm: The the conventional k-means algorithm is surprisingly simple and requires only a few simple steps. During the first step the algorithm randomly chooses k centroids, while k being equal to the number of clusters one chooses and centroids being data points representing the center of a cluster. The second step is actually a two-step processes called expectation-maximization, which is repeated until the positions of the centroids do not change anymore. First, during the expectation step of the second step, each data point is assigned to its nearest centroid. Then, during the maximization step of the second step, the mean of all data points is calculated for each cluster and a new centroid is set accordingly. Interestingly enough the quality of the cluster assignments is determined by computing the sum of the squared Euclidean distances of each data point to its closest centroid (sum of the squared error). Since the goal is to try to maximize the quality of the cluster assignments, the algorithm tries to minimize the error. Simple, but effective and not unlike what we are doing, when conducting a regression analysis. Below you can see what a conventional k-means algorithm looks like: 1: Specify the number k clusters to assign. 2: Randomly initialize k centroids. 3: repeat 4: – expectation: Assign each data point to its closest centroid. 5: – maximization: Compute the new centroid (mean) of each cluster. 6: until The centroid positions do not change. During my research I have come across the Python module scikit-learn and chose to use said module to replicate the clustering method adapted from Baumer et al. (2017) (see above) using Python. kmeans = KMeans(init=&quot;random&quot;, n_clusters=3, n_init=10, max_iter=300, random_state=15) kmeans.fit(iris_ml_std) #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}KMeans(init=&#x27;random&#x27;, n_clusters=3, random_state=15)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(init=&#x27;random&#x27;, n_clusters=3, random_state=15) cl_kmeans_lowest_SSE = kmeans.inertia_ cl_kmeans_centers = kmeans.cluster_centers_ cl_kmeans_req_it = kmeans.n_iter_ print(&quot;The lowest SSE value: &quot;, cl_kmeans_lowest_SSE) ## The lowest SSE value: 141.15417813388655 print(&quot;Final locations of the centroids: &quot;, &quot;\\n&quot;, cl_kmeans_centers) ## Final locations of the centroids: ## [[ 1.16743407 0.15377779 1.00314548 1.02963256] ## [-1.01457897 0.84230679 -1.30487835 -1.25512862] ## [-0.01139555 -0.87288504 0.37688422 0.31165355]] print(&quot;The number of iterations required to converge: &quot;, cl_kmeans_req_it) ## The number of iterations required to converge: 4 cl_labels = kmeans.labels_ cl_labels = cl_labels.reshape(-1, 1) # Add cluster assignments to &#39;iris_arr&#39; iris_arr_cl = iris_arr.copy() iris_arr_cl = np.concatenate((iris_arr_cl, cl_labels), axis=1) # Add cluster assignments to &#39;iris_df&#39; iris_df_cl = iris_df.copy() cl_labels_str = cl_labels.copy() cl_labels_str = cl_labels_str.astype(str) iris_df_cl[&quot;Cluster&quot;] = cl_labels_str The method which I used to conduct the k-means clustering in Python I adapted from an online tutorial by Arvai (2022). 3.2.4 Visualization of Results of Clustering Python iris_c_plot_1_py = px.scatter( iris_df_cl, x=&quot;TCategory&quot;, y=&quot;Sepal_length&quot;, color=&quot;Cluster&quot;, labels=dict( TCategory=&quot;Species&quot;, Sepal_length=&quot;Sepal Length (cm)&quot;), template=&quot;plotly_white&quot;) iris_c_plot_1_py.update_layout( xaxis= dict( tickmode=&quot;array&quot;, tickvals=[0,1,2], ticktext=[&quot;Iris Setosa&quot;,&quot;Iris Versicolor&quot;,&quot;Iris Virginica&quot;])) Figure 3.5: Categorization vs. Clustering - plotted against Sepal Length using *Python* iris_c_plot_2_py = px.scatter( iris_df_cl, x=&quot;TCategory&quot;, y=&quot;Sepal_width&quot;, color=&quot;Cluster&quot;, labels=dict( TCategory=&quot;Species&quot;, Sepal_width=&quot;Sepal Width (cm)&quot;), template=&quot;plotly_white&quot;) iris_c_plot_2_py.update_layout( xaxis= dict( tickmode=&quot;array&quot;, tickvals=[0,1,2], ticktext=[&quot;Iris Setosa&quot;,&quot;Iris Versicolor&quot;,&quot;Iris Virginica&quot;])) Figure 3.6: Categorization vs. Clustering - plotted against Sepal Width using *Python* iris_c_plot_3_py = px.scatter( iris_df_cl, x=&quot;TCategory&quot;, y=&quot;Petal_length&quot;, color=&quot;Cluster&quot;, labels=dict( TCategory=&quot;Species&quot;, Petal_length=&quot;Petal Length (cm)&quot;), template=&quot;plotly_white&quot;) iris_c_plot_3_py.update_layout( xaxis= dict( tickmode=&quot;array&quot;, tickvals=[0,1,2], ticktext=[&quot;Iris Setosa&quot;,&quot;Iris Versicolor&quot;,&quot;Iris Virginica&quot;])) Figure 3.7: Categorization vs. Clustering - plotted against Petal Length using *Python* iris_c_plot_4_py = px.scatter( iris_df_cl, x=&quot;TCategory&quot;, y=&quot;Petal_width&quot;, color=&quot;Cluster&quot;, labels=dict( TCategory=&quot;Species&quot;, Petal_width=&quot;Petal Width (cm)&quot;), template=&quot;plotly_white&quot;) iris_c_plot_4_py.update_layout( xaxis= dict( tickmode=&quot;array&quot;, tickvals=[0,1,2], ticktext=[&quot;Iris Setosa&quot;,&quot;Iris Versicolor&quot;,&quot;Iris Virginica&quot;])) Figure 3.8: Categorization vs. Clustering - plotted against Petal Width using *Python* 3.2.5 Conclusion Figures one throughfour clearly show, that the method, which I adapated from Bauner et al. (2017) yields surprisingly good results, given the data the calculations are based on. Comparing the figures mentioned above with the figures five through eight, it becomes clear that the “Python method”, which I adapted from Arvai (2022), yields surprisingly good results as well. Nevertheless I find it import to note that the “Python method” and the “R method” yield slightly different results. Hence further inspection of said differences is necessary. References Arvai, K. (2022). K-means clustering in python: A practical guide. In Real Python. Real Python. https://realpython.com/k-means-clustering-python/#how-to-perform-k-means-clustering-in-python Baumer, D. T., Benjamin S.and Kaplan, &amp; Horton, N. J. (2017). Modern data science with r. CRC Press. https://doi.org/10.1201/9781315113760 "],["supervised-learning.html", "Chapter 4 Supervised Learning 4.1 Introduction to Decision Trees", " Chapter 4 Supervised Learning 4.1 Introduction to Decision Trees 4.1.1 What are Decision Trees? Decision trees are non-parametric supervised learning algorithms, that are used to predict the class or value of a specific target variable. Predictions are based on n simple decision rules which are inferred from the set of data, which was used to train the respective algorithm (Chauhan, 2022). Hastie et al. (2009) differentiate between to classes of decision trees: Regression trees and classification trees. The former is a class of decision tree algorithms, that are used when the target variable is continuous; accordingly algorithms of the latter class are used, when the target variable is categorical. To simplify matters this chapter shall focus on classification trees. In contrast to other decision algorithms decision trees are non-compensatory. Decision algorithms, such as random forests and regression, which are typically compensatory algorithms, are designed to use most, if not all, of the available cue information. The design of such algorithms is based on the premise, that the value of one cue (a.k.a. feature or predictor) could overturn the evidence given by another or several other cues. Non-compensatory algorithms on the hand, such as decision trees, use only a partial subset of the given cue information to reach a decision. This design is based on the premise, that the value or values of one or several cues cannot be outweighed by the values of any other cues. In short this means that decision trees deliberately ignore information. This design can actually offer significant practical and statistical advantages (Phillips et al., 2017). 4.1.2 Basic Concepts of Decision Trees Classification trees are used to solve binary classification tasks. The goal of tasks of this class is the prediction of a binary criterion value (e.g. having heart disease vs. not having heart disease) for each of a set of individual cases (e.g. patients) based on each case’s values on a not necessarily specified range of cues (e.g. thallium scintigraphy results, chest pain type etc.) (Phillips et al., 2017). These kinds of decision trees (as well as decision trees in general) can be applied as an ordered set of n simple conditional rules (A ⟹ B). These rules are applied sequentially (Phillips et al., 2017). 4.1.3 A Short History of Decision Trees One of the first decision tree algorithms was actually invented to model human learning in psychology (Hunt et al., 1966). This algorithm forms the foundation for many popular decision tree algorithms such as the ID3 algorithm (Quinlan, 1986), the C4.5 algorithm (Quinlan, 2003) and the famous CART (Classification And Regression Trees) algorithm (Breiman, 1984). For further information I recommend reading the short but very informative article “Decision Trees” by de Ville (2013). 4.1.4 Basic Terminology of Decision Trees Before we dive in deeper in the inner workings of decision trees I would like to give a short overview of the basic terminology used in the context of decision trees. Formally a decision tree is comprised of the following elements (Chauhan, 2022; Phillips et al., 2017; “What Is a Decision Tree,” n.d.): The Root Node … is the top node of a decision tree. has no incoming branches. represents the entire population or sample. A Decision Node … is a sub-node (i.e. not a root node), that splits into further sub-nodes. represents cue-based questions. represents a subset of the data. Branches … represent answers to cue-based questions. Parent nodes … are nodes, that split into sub-nodes. Child nodes … are the sub-nodes of parent nodes. Leaf or terminal nodes … do not split into further sub-nodes. represent decisions. A Sub-tree … is a sub-section of the entire tree. 4.1.5 An Example of Creating a Decision with Python Below you see an example of a decision tree, which I created using a free software machine learning library for the Python programming language called scikit-learn. The algorithm used for the creation of the following decision tree is based on the CART algorithm (Breiman, 1984). #|label: dt-python #| tidy: true #| tidy-opts: #| - width.cutoff=60 iris = load_iris() X, y = iris.data, iris.target clf = tree.DecisionTreeClassifier() clf = clf.fit(X, y) tree.plot_tree(clf) Unfortunately “the scikit-learn implementation does not support categorical variables for now” (“Decision Trees,” n.d.). Luckily the are R packages that allow the construction of decision trees based on categorical data. An example would be the rpart package. 4.1.6 An Example of Creating a Decision with R Below you see an example of a decision tree, which I created using the rpart package. The algorithm used for the creation of the following decision tree is based on the CART algorithm (Breiman, 1984). 4.1.6.1 Preprocessing of Data set.seed(678) path &lt;- &#39;https://raw.githubusercontent.com/guru99-edu/R-Programming/master/titanic_data.csv&#39; titanic &lt;-read.csv(path) shuffle_index &lt;- sample(1:nrow(titanic)) titanic &lt;- titanic[shuffle_index, ] preclean_titanic &lt;- titanic preclean_titanic$age &lt;- as.integer(preclean_titanic$age) ## Warning: NAs durch Umwandlung erzeugt preclean_titanic$fare &lt;- as.integer(preclean_titanic$fare) ## Warning: NAs durch Umwandlung erzeugt clean_titanic &lt;- preclean_titanic %&gt;% select(-c(home.dest, cabin, name, x, ticket)) %&gt;% # Drop variables mutate(pclass = factor(pclass, levels = c(1, 2, 3), #Convert to factor level labels = c(&#39;Upper&#39;, &#39;Middle&#39;, &#39;Lower&#39;)), survived = factor(survived, levels = c(0, 1), #Convert to factor level labels = c(&#39;No&#39;, &#39;Yes&#39;)), sex = factor(sex), embarked = factor(embarked)) %&gt;% na.omit() 4.1.6.2 Creating a Train and Test Dataset create_train_test &lt;- function(data, size = 0.8, train = TRUE) { n_row = nrow(data) total_row = size * n_row train_sample &lt;- 1:total_row if (train == TRUE) { return(data[train_sample, ]) } else { return(data[-train_sample, ]) } } data_train &lt;- create_train_test(clean_titanic, 0.8, train = TRUE) # Train dataset with 80% of original data data_test &lt;- create_train_test(clean_titanic, 0.8, train = FALSE) # test dataset with 20% of original data 4.1.6.3 Creating and Visualizing the Decision Tree fit &lt;- rpart(survived ~ ., data = data_train, method = &quot;class&quot;) rpart.plot(fit, extra = 106) 4.1.6.4 Prediction of Data predict_unseen &lt;- predict(fit, data_test, type = &quot;class&quot;) table_mat &lt;- table(data_test$survived, predict_unseen) table_mat ## predict_unseen ## No Yes ## No 105 11 ## Yes 34 59 4.1.7 Advantages and Disadvantages of Decision Trees One of the big advantages of decision trees is, that they are incredibly simple to understand, to interpret and to visualize. Furthermore they are generally able to handle both numerical and categorical data. Another advantage is, that they can not only solve classification tasks, but regression tasks as well. Besides that they can even handle multi-output problems (problems where several outputs need to be predicted). Unfortunately, like any other algorithms or statistical methods, decision tree algorithms do have several disadvantages as well. The main and most important disadvantage of decision trees though is the problem of overfitting. As I have explained before decision tree algorithms are non-compensatory algorithms, i.e. they ignore data (see above). This does not mean though, that they are always simple. Quite the opposite in fact. Without the appropriate necessary restrictions decision trees can become highly complex networks of questions containing dozens or - depending on the respective dataset - even hundreds of dozens of nodes. Although such complex decision trees usually describe the data, which they were trained with, very well, they tend to be exceptionally bad at predicting data. Fortunately the problem of overfitting can be overcome by carefully pruning - i.e. trimming off certain branches of the decision tree - without decreasing the overall accuracy of the decision tree algorithm. One algorithm used to achieve this is the minimal cost-complexity pruning algorithm. For further information on the advantages and disadvantages of decision trees please read the respective article on the scikit-learn website (“Decision Trees,” n.d.). 4.1.8 Fast-and-Frugal Trees Another solution for the problem of overfitting is the usage of more restrictive forms of decision tree algorithms. One of the most restrictive forms of a decision tree is a fast-and-frugal tree (Phillips et al., 2017). Based on the research by Gigerenzer and colleagues on the topic of heuristics Phillips, Neth (University of Constance), Woike and Gaissmaier (University of Constance) (2017) build the R package FFTrees, that allows users to easily create, visualize, and evaluate fast-and-frugal trees. Furthermore the package introduces a very handy new class of algorithms for constructing fast-and-frugal trees. 4.1.9 Creating a FFT with FFTrees # Step 1: Create FFTs from training data and test on testing data heart. heart.fft &lt;- FFTrees(formula = diagnosis ~ ., # Criterion data = heart.train, # Training data data.test = heart.test, # Testing data main = &quot;Heart Disease&quot;, # Optional labels decision.labels = c(&quot;Low-Risk&quot;, &quot;High-Risk&quot;)) # Step 1: Create FFTs from training data and test on testing data heart. heart.fft &lt;- FFTrees(formula = diagnosis ~ ., # Criterion data = heart.train, # Training data data.test = heart.test, # Testing data main = &quot;Heart Disease&quot;, # Optional labels decision.labels = c(&quot;Low-Risk&quot;, &quot;High-Risk&quot;)) ## Setting &#39;goal = bacc&#39; ## Setting &#39;goal.chase = bacc&#39; ## Setting &#39;goal.threshold = bacc&#39; ## Setting cost.outcomes = list(hi = 0, mi = 1, fa = 1, cr = 0) ## Growing FFTs with ifan: ## [========================================&gt;------------------------------------] 54% [==============================================&gt;------------------------------] 62% [====================================================&gt;------------------------] 69% [==========================================================&gt;------------------] 77% [================================================================&gt;------------] 85% [======================================================================&gt;------] 92% [=============================================================================] 100% ## Fitting other algorithms for comparison (disable with do.comp = FALSE) ... # Step 3: Inspect and summarize FFTs heart.fft # Print statistics of the final FFT ## Heart Disease ## FFTrees ## - Trees: 7 fast-and-frugal trees predicting diagnosis ## - Outcome costs: [hi = 0, mi = 1, fa = 1, cr = 0] ## ## FFT #1: Definition ## [1] If thal = {rd,fd}, decide High-Risk. ## [2] If cp != {a}, decide Low-Risk. ## [3] If ca &gt; 0, decide High-Risk, otherwise, decide Low-Risk. ## ## FFT #1: Training Accuracy ## Training data: N = 150, Pos (+) = 66 (44%) ## ## | | True + | True - | Totals: ## |----------|--------|--------| ## | Decide + | hi 54 | fa 18 | 72 ## | Decide - | mi 12 | cr 66 | 78 ## |----------|--------|--------| ## Totals: 66 84 N = 150 ## ## acc = 80.0% ppv = 75.0% npv = 84.6% ## bacc = 80.2% sens = 81.8% spec = 78.6% ## ## FFT #1: Training Speed, Frugality, and Cost ## mcu = 1.74, pci = 0.87, E(cost) = 0.200 inwords(heart.fft) # Print a verbal description of the final FFT ## [1] &quot;If thal = {rd,fd}, decide High-Risk.&quot; ## [2] &quot;If cp != {a}, decide Low-Risk.&quot; ## [3] &quot;If ca &gt; 0, decide High-Risk, otherwise, decide Low-Risk.&quot; summary(heart.fft) # Print statistics of all FFTs ## Heart Disease ## ## FFTrees ## - Trees: 7 fast-and-frugal trees predicting diagnosis ## - Parameters: algorithm = &#39;ifan&#39;, goal = &#39;bacc&#39;, goal.chase = &#39;bacc&#39;, ## sens.w = 0.5, max.levels = 4 ## ## ## Table: (\\#tab:fft)Tree definitions ## ## | tree| nodes|classes |cues |directions |thresholds |exits | ## |----:|-----:|:-------|:----------------|:----------|:-------------------|:---------| ## | 1| 3|c;c;n |thal;cp;ca |=;=;&gt; |rd,fd;a;0 |1;0;0.5 | ## | 2| 4|c;c;n;c |thal;cp;ca;slope |=;=;&gt;;= |rd,fd;a;0;flat,down |1;0;1;0.5 | ## | 3| 3|c;c;n |thal;cp;ca |=;=;&gt; |rd,fd;a;0 |0;1;0.5 | ## | 4| 4|c;c;n;c |thal;cp;ca;slope |=;=;&gt;;= |rd,fd;a;0;flat,down |1;1;0;0.5 | ## | 5| 3|c;c;n |thal;cp;ca |=;=;&gt; |rd,fd;a;0 |0;0;0.5 | ## | 6| 4|c;c;n;c |thal;cp;ca;slope |=;=;&gt;;= |rd,fd;a;0;flat,down |0;0;0;0.5 | ## | 7| 4|c;c;n;c |thal;cp;ca;slope |=;=;&gt;;= |rd,fd;a;0;flat,down |1;1;1;0.5 | ## ## ## Table: (\\#tab:fft)Tree statistics on training data ## ## | tree| n| hi| fa| mi| cr| sens| spec| far| ppv| npv| acc| bacc| wacc| cost_decisions| cost_cues| cost| pci| mcu| ## |----:|---:|--:|--:|--:|--:|----:|----:|----:|----:|----:|----:|----:|----:|--------------:|---------:|----:|----:|----:| ## | 1| 150| 54| 18| 12| 66| 0.82| 0.79| 0.21| 0.75| 0.85| 0.80| 0.80| 0.80| 0.20| 0| 0.20| 0.87| 1.74| ## | 2| 150| 57| 22| 9| 62| 0.86| 0.74| 0.26| 0.72| 0.87| 0.79| 0.80| 0.80| 0.21| 0| 0.21| 0.86| 1.84| ## | 3| 150| 44| 7| 22| 77| 0.67| 0.92| 0.08| 0.86| 0.78| 0.81| 0.79| 0.79| 0.19| 0| 0.19| 0.88| 1.56| ## | 4| 150| 60| 31| 6| 53| 0.91| 0.63| 0.37| 0.66| 0.90| 0.75| 0.77| 0.77| 0.25| 0| 0.25| 0.84| 2.12| ## | 5| 150| 28| 2| 38| 82| 0.42| 0.98| 0.02| 0.93| 0.68| 0.73| 0.70| 0.70| 0.27| 0| 0.27| 0.87| 1.70| ## | 6| 150| 21| 1| 45| 83| 0.32| 0.99| 0.01| 0.95| 0.65| 0.69| 0.65| 0.65| 0.31| 0| 0.31| 0.85| 1.90| ## | 7| 150| 64| 56| 2| 28| 0.97| 0.33| 0.67| 0.53| 0.93| 0.61| 0.65| 0.65| 0.39| 0| 0.39| 0.82| 2.30| ## ## ## Table: (\\#tab:fft)Tree statistics on test data ## ## | tree| n| hi| fa| mi| cr| sens| spec| far| ppv| npv| acc| bacc| wacc| cost_decisions| cost_cues| cost| pci| mcu| ## |----:|---:|--:|--:|--:|--:|----:|----:|----:|----:|----:|----:|----:|----:|--------------:|---------:|----:|----:|----:| ## | 1| 153| 64| 19| 9| 61| 0.88| 0.76| 0.24| 0.77| 0.87| 0.82| 0.82| 0.82| 0.18| 0| 0.18| 0.87| 1.73| ## | 2| 153| 67| 26| 6| 54| 0.92| 0.68| 0.32| 0.72| 0.90| 0.79| 0.80| 0.80| 0.21| 0| 0.21| 0.86| 1.85| ## | 3| 153| 49| 8| 24| 72| 0.67| 0.90| 0.10| 0.86| 0.75| 0.79| 0.79| 0.79| 0.21| 0| 0.21| 0.87| 1.63| ## | 4| 153| 69| 36| 4| 44| 0.95| 0.55| 0.45| 0.66| 0.92| 0.74| 0.75| 0.75| 0.26| 0| 0.26| 0.85| 1.95| ## | 5| 153| 28| 0| 45| 80| 0.38| 1.00| 0.00| 1.00| 0.64| 0.71| 0.69| 0.69| 0.29| 0| 0.29| 0.86| 1.78| ## | 6| 153| 22| 0| 51| 80| 0.30| 1.00| 0.00| 1.00| 0.61| 0.67| 0.65| 0.65| 0.33| 0| 0.33| 0.85| 1.97| ## | 7| 153| 72| 56| 1| 24| 0.99| 0.30| 0.70| 0.56| 0.96| 0.63| 0.64| 0.64| 0.37| 0| 0.37| 0.84| 2.11| # Step 4: Visualize the final FFT and performance results # a) plot final FFT applied to test data: plot(heart.fft, data = &quot;test&quot;) # b) plot individual cue accuracies in ROC space: plot(heart.fft, what = &quot;cues&quot;) ## Using cue training statistics of object x: ## Cue accuracies ranked by bacc 4.1.10 Conclusion Decision tree algorithms are incredibly versatile and can be used in a number of contexts for wide range of problems. Due to their simplicity they are easy to understand, to interpret and to visualize. As well as any other algorithms they do have certain libations of course. Thanks to the work for example of scientists like Phillips et al. (2017), many of these limitations can be overcome though. Fast-and-frugal trees especially have an amazing potential in a number of fields. Thus I recommend everyone to familiarize themselves with the usage of decision trees and to take a stroll though this algorithmic jungle. References Breiman, L. (1984). Classification and regression trees. Wadsworth Internat. Group. Chauhan, N. S. (2022). Decision tree algorithm, explained. In KDnuggets. KDnuggets. https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html Decision trees. (n.d.). In scikit-learn. scikit-learn. Retrieved November 30, 2022, from https://scikit-learn.org/stable/modules/tree.html Hastie, T., Tibshirani, R., &amp; Friedman, J. H. (2009). The elements of statistical learning: Data mining, inference, and prediction (2. ed.). Springer. Hunt, E. B., Marin, J., &amp; Stone, P. J. (1966). Experiments in induction. Academic Pr. Phillips, N. D., Neth, H., Woike, J. K., &amp; Gaissmaier, W. (2017). FFTrees: A toolbox to create, visualize, and evaluate fast-and-frugal decision trees. Judgment and Decision Making, 12(4), 344–368. Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1(1), 81–106. https://doi.org/10.1007/BF00116251 Quinlan, J. R. (2003). C4.5: Programs for machine learning (5. [pr.]). Morgan Kaufmann. Ville, B. de. (2013). WIREs Computational Statistics, 5(6), 448–455. https://doi.org/https://doi.org/10.1002/wics.1278 What is a decision tree. (n.d.). In IBM. IBM. Retrieved November 30, 2022, from https://www.ibm.com/topics/decision-trees "],["works-cited.html", "Works Cited", " Works Cited Arvai, K. (2022). K-means clustering in python: A practical guide. In Real Python. Real Python. https://realpython.com/k-means-clustering-python/#how-to-perform-k-means-clustering-in-python Baumer, D. T., Benjamin S.and Kaplan, &amp; Horton, N. J. (2017). Modern data science with r. CRC Press. https://doi.org/10.1201/9781315113760 Breiman, L. (1984). Classification and regression trees. Wadsworth Internat. Group. Chauhan, N. S. (2022). Decision tree algorithm, explained. In KDnuggets. KDnuggets. https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html Decision trees. (n.d.). In scikit-learn. scikit-learn. Retrieved November 30, 2022, from https://scikit-learn.org/stable/modules/tree.html Hastie, T., Tibshirani, R., &amp; Friedman, J. H. (2009). The elements of statistical learning: Data mining, inference, and prediction (2. ed.). Springer. Hunt, E. B., Marin, J., &amp; Stone, P. J. (1966). Experiments in induction. Academic Pr. Phillips, N. D., Neth, H., Woike, J. K., &amp; Gaissmaier, W. (2017). FFTrees: A toolbox to create, visualize, and evaluate fast-and-frugal decision trees. Judgment and Decision Making, 12(4), 344–368. Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1(1), 81–106. https://doi.org/10.1007/BF00116251 Quinlan, J. R. (2003). C4.5: Programs for machine learning (5. [pr.]). Morgan Kaufmann. Ville, B. de. (2013). WIREs Computational Statistics, 5(6), 448–455. https://doi.org/https://doi.org/10.1002/wics.1278 What is a decision tree. (n.d.). In IBM. IBM. Retrieved November 30, 2022, from https://www.ibm.com/topics/decision-trees "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
