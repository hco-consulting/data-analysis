<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Supervised Learning | Advanced Data Analysis in R and Python</title>
  <meta name="description" content="Chapter 4 Supervised Learning | Advanced Data Analysis in R and Python" />
  <meta name="generator" content="bookdown 0.30.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Supervised Learning | Advanced Data Analysis in R and Python" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="hco-consulting/data-analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Supervised Learning | Advanced Data Analysis in R and Python" />
  
  
  

<meta name="author" content="Hamilkar Constantin Oueslati" />


<meta name="date" content="2022-11-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="unsupervised-learning.html"/>
<link rel="next" href="works-cited.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contact-the-author"><i class="fa fa-check"></i>Contact the Author</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prerequisites.html"><a href="prerequisites.html#load-r-packages"><i class="fa fa-check"></i><b>1.1</b> Load R Packages</a></li>
<li class="chapter" data-level="1.2" data-path="prerequisites.html"><a href="prerequisites.html#load-python-version-3.10.8"><i class="fa fa-check"></i><b>1.2</b> Load Python Version 3.10.8</a></li>
<li class="chapter" data-level="1.3" data-path="prerequisites.html"><a href="prerequisites.html#import-python-modules"><i class="fa fa-check"></i><b>1.3</b> Import Python Modules</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="brushing-up-on-the-basics.html"><a href="brushing-up-on-the-basics.html"><i class="fa fa-check"></i><b>2</b> Brushing Up on the Basics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="brushing-up-on-the-basics.html"><a href="brushing-up-on-the-basics.html#mixed-anova-r"><i class="fa fa-check"></i><b>2.1</b> Mixed ANOVA <em>R</em></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="brushing-up-on-the-basics.html"><a href="brushing-up-on-the-basics.html#preperation-of-dataset-iris-r"><i class="fa fa-check"></i><b>2.1.1</b> Preperation of Dataset <code>iris</code> <em>R</em></a></li>
<li class="chapter" data-level="2.1.2" data-path="brushing-up-on-the-basics.html"><a href="brushing-up-on-the-basics.html#creating-the-anova-model"><i class="fa fa-check"></i><b>2.1.2</b> Creating the ANOVA Model</a></li>
<li class="chapter" data-level="2.1.3" data-path="brushing-up-on-the-basics.html"><a href="brushing-up-on-the-basics.html#testing-assumptions-anova-r"><i class="fa fa-check"></i><b>2.1.3</b> Testing Assumptions of the ANOVA <em>R</em></a></li>
<li class="chapter" data-level="2.1.4" data-path="brushing-up-on-the-basics.html"><a href="brushing-up-on-the-basics.html#results-mixed-anova"><i class="fa fa-check"></i><b>2.1.4</b> Results of Mixed ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>3</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#non-hierarchical-clustering-r"><i class="fa fa-check"></i><b>3.1</b> Non-Hierarchical Clustering <em>R</em></a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#import-dataset-iris-r"><i class="fa fa-check"></i><b>3.1.1</b> Import Dataset <code>iris</code> <em>R</em></a></li>
<li class="chapter" data-level="3.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-k-means-algorithm"><i class="fa fa-check"></i><b>3.1.2</b> Clustering Using a <em>k</em>-means Algorithm</a></li>
<li class="chapter" data-level="3.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#visualization-results-clustering-r"><i class="fa fa-check"></i><b>3.1.3</b> Visualization of Results of Clustering <em>R</em></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#non-hierarchical-clustering-python"><i class="fa fa-check"></i><b>3.2</b> Non-Hierarchical Clustering <em>Python</em></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#import-dataset-iris-python"><i class="fa fa-check"></i><b>3.2.1</b> Import Dataset <code>iris</code> <em>Python</em></a></li>
<li class="chapter" data-level="3.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#preprocessing-of-data"><i class="fa fa-check"></i><b>3.2.2</b> Preprocessing of Data</a></li>
<li class="chapter" data-level="3.2.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-using-a-k-means-algorithm"><i class="fa fa-check"></i><b>3.2.3</b> Clustering Using a <em>k</em>-means Algorithm</a></li>
<li class="chapter" data-level="3.2.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#visualization-results-clustering-python"><i class="fa fa-check"></i><b>3.2.4</b> Visualization of Results of Clustering <em>Python</em></a></li>
<li class="chapter" data-level="3.2.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#conclusion-k-means"><i class="fa fa-check"></i><b>3.2.5</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>4</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="supervised-learning.html"><a href="supervised-learning.html#into-decision-trees"><i class="fa fa-check"></i><b>4.1</b> Introduction to Decision Trees</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#what-dt"><i class="fa fa-check"></i><b>4.1.1</b> What are Decision Trees?</a></li>
<li class="chapter" data-level="4.1.2" data-path="supervised-learning.html"><a href="supervised-learning.html#concept-dt"><i class="fa fa-check"></i><b>4.1.2</b> Basic Concepts of Decision Trees</a></li>
<li class="chapter" data-level="4.1.3" data-path="supervised-learning.html"><a href="supervised-learning.html#history-dt"><i class="fa fa-check"></i><b>4.1.3</b> A Short History of Decision Trees</a></li>
<li class="chapter" data-level="4.1.4" data-path="supervised-learning.html"><a href="supervised-learning.html#terminology-dt"><i class="fa fa-check"></i><b>4.1.4</b> Basic Terminology of Decision Trees</a></li>
<li class="chapter" data-level="4.1.5" data-path="supervised-learning.html"><a href="supervised-learning.html#eg-dt-python"><i class="fa fa-check"></i><b>4.1.5</b> An Example of Creating a Decision Tree with <em>Python</em></a></li>
<li class="chapter" data-level="4.1.6" data-path="supervised-learning.html"><a href="supervised-learning.html#eg-dt-r"><i class="fa fa-check"></i><b>4.1.6</b> An Example of Creating a Decision Tree with <em>R</em></a></li>
<li class="chapter" data-level="4.1.7" data-path="supervised-learning.html"><a href="supervised-learning.html#adv-disadv-dt"><i class="fa fa-check"></i><b>4.1.7</b> Advantages and Disadvantages of Decision Trees</a></li>
<li class="chapter" data-level="4.1.8" data-path="supervised-learning.html"><a href="supervised-learning.html#fft"><i class="fa fa-check"></i><b>4.1.8</b> Fast-and-Frugal Trees</a></li>
<li class="chapter" data-level="4.1.9" data-path="supervised-learning.html"><a href="supervised-learning.html#fft-creation"><i class="fa fa-check"></i><b>4.1.9</b> Creating a FFT with <code>FFTrees</code></a></li>
<li class="chapter" data-level="4.1.10" data-path="supervised-learning.html"><a href="supervised-learning.html#dt-conclusion"><i class="fa fa-check"></i><b>4.1.10</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="works-cited.html"><a href="works-cited.html"><i class="fa fa-check"></i>Works Cited</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Data Analysis in R and Python</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Supervised Learning<a href="supervised-learning.html#supervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="into-decision-trees" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction to Decision Trees<a href="supervised-learning.html#into-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="what-dt" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> What are Decision Trees?<a href="supervised-learning.html#what-dt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Decision trees are non-parametric supervised learning algorithms, that are used to predict the class or value of a specific target variable. Predictions are based on <em>n</em> simple decision rules which are inferred from the set of data, which was used to train the respective algorithm <span class="citation">(<a href="#ref-KDChauhan2022" role="doc-biblioref">Chauhan, 2022</a>)</span>.</p>
<p>Hastie et al. <span class="citation">(<a href="#ref-Hastie2009" role="doc-biblioref">2009</a>)</span> differentiate between to classes of decision trees: <em>Regression trees</em> and <em>classification trees</em>. The former is a class of decision tree algorithms, that are used when the target variable is <em>continuous</em>; accordingly algorithms of the latter class are used, when the target variable is <em>categorical</em>. To simplify matters this chapter shall focus on <em>classification trees</em>.</p>
<p>In contrast to other decision algorithms decision trees are <em>non-compensatory</em>. Decision algorithms, such as <em>random forests</em> and <em>regression</em>, which are typically compensatory algorithms, are designed to use most, if not all, of the available cue information. The design of such algorithms is based on the premise, that the value of one cue (a.k.a. feature or predictor) could overturn the evidence given by another or several other cues. Non-compensatory algorithms on the hand, such as decision trees, use only a partial subset of the given cue information to reach a decision. This design is based on the premise, that the value or values of one or several cues cannot be outweighed by the values of any other cues. In short this means that decision trees deliberately ignore information. This design can actually offer significant practical and statistical advantages <span class="citation">(<a href="#ref-Neth2017" role="doc-biblioref">Phillips et al., 2017</a>)</span>.</p>
</div>
<div id="concept-dt" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Basic Concepts of Decision Trees<a href="supervised-learning.html#concept-dt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Classification trees are used to solve <em>binary classification tasks</em>. The goal of tasks of this class is the prediction of a <em>binary criterion value</em> (e.g. having heart disease vs. not having heart disease) for each of a set of <em>individual cases</em> (e.g. patients) based on each case’s values on a not necessarily specified range of <em>cues</em> (e.g. thallium scintigraphy results, chest pain type etc.) <span class="citation">(<a href="#ref-Neth2017" role="doc-biblioref">Phillips et al., 2017</a>)</span>.</p>
<p>These kinds of decision trees (as well as decision trees in general) can be applied as an ordered set of <em>n</em> simple conditional rules (A ⟹ B). These rules are applied sequentially <span class="citation">(<a href="#ref-Neth2017" role="doc-biblioref">Phillips et al., 2017</a>)</span>.</p>
</div>
<div id="history-dt" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> A Short History of Decision Trees<a href="supervised-learning.html#history-dt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the first decision tree algorithms was actually invented to model human learning in psychology <span class="citation">(<a href="#ref-Hunt1966" role="doc-biblioref">Hunt et al., 1966</a>)</span>. This algorithm forms the foundation for many popular decision tree algorithms such as the <strong>ID3</strong> algorithm <span class="citation">(<a href="#ref-Quinlan1986" role="doc-biblioref">Quinlan, 1986</a>)</span>, the <strong>C4.5</strong> algorithm <span class="citation">(<a href="#ref-Quinlan2003" role="doc-biblioref">Quinlan, 2003</a>)</span> and the famous <strong>CART</strong> (<strong>C</strong>lassification <strong>A</strong>nd <strong>R</strong>egression <strong>T</strong>rees) algorithm <span class="citation">(<a href="#ref-Breiman1984" role="doc-biblioref">Breiman, 1984</a>)</span>.</p>
<p>For further information I recommend reading the short but very informative article “Decision Trees” by de Ville <span class="citation">(<a href="#ref-deVille2013" role="doc-biblioref">2013</a>)</span>.</p>
</div>
<div id="terminology-dt" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Basic Terminology of Decision Trees<a href="supervised-learning.html#terminology-dt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we dive in deeper in the inner workings of decision trees I would like to give a short overview of the basic terminology used in the context of decision trees.</p>
<p>Formally a decision tree is comprised of the following elements <span class="citation">(<a href="#ref-KDChauhan2022" role="doc-biblioref">Chauhan, 2022</a>; <a href="#ref-Neth2017" role="doc-biblioref">Phillips et al., 2017</a>; <a href="#ref-ibm" role="doc-biblioref"><span>“What Is a Decision Tree,”</span> n.d.</a>)</span>:</p>
<ul>
<li><p>The <em>Root Node</em> …</p>
<ul>
<li><p>is the top node of a decision tree.</p></li>
<li><p>has no incoming branches.</p></li>
<li><p>represents the entire population or sample.</p></li>
</ul></li>
<li><p>A <em>Decision Node</em> …</p>
<ul>
<li><p>is a sub-node (i.e. not a root node), that splits into further sub-nodes.</p></li>
<li><p>represents cue-based questions.</p></li>
<li><p>represents a subset of the data.</p></li>
</ul></li>
<li><p><em>Branches</em> …</p>
<ul>
<li>represent answers to cue-based questions.</li>
</ul></li>
<li><p><em>Parent</em> <em>nodes</em> …</p>
<ul>
<li>are nodes, that split into sub-nodes.</li>
</ul></li>
<li><p><em>Child nodes</em> …</p>
<ul>
<li>are the sub-nodes of parent nodes.</li>
</ul></li>
<li><p><em>Leaf</em> or <em>terminal node</em>s <em>…</em></p>
<ul>
<li><p>do not split into further sub-nodes.</p></li>
<li><p>represent decisions.</p></li>
</ul></li>
<li><p>A S<em>ub-tree</em> …</p>
<ul>
<li>is a sub-section of the entire tree.</li>
</ul></li>
</ul>
</div>
<div id="eg-dt-python" class="section level3 hasAnchor" number="4.1.5">
<h3><span class="header-section-number">4.1.5</span> An Example of Creating a Decision Tree with <em>Python</em><a href="supervised-learning.html#eg-dt-python" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below you see an example of a decision tree, which I created using a free software machine learning library for the Python programming language called <a href="https://scikit-learn.org"><em>scikit-learn</em></a>.</p>
<p>The algorithm used for the creation of the following decision tree is based on the <strong>CART</strong> algorithm <span class="citation">(<a href="#ref-Breiman1984" role="doc-biblioref">Breiman, 1984</a>)</span>.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="supervised-learning.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co">#|label: dt-python</span></span>
<span id="cb55-2"><a href="supervised-learning.html#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co">#| tidy: true</span></span>
<span id="cb55-3"><a href="supervised-learning.html#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co">#| tidy-opts: </span></span>
<span id="cb55-4"><a href="supervised-learning.html#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - width.cutoff=60</span></span>
<span id="cb55-5"><a href="supervised-learning.html#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="supervised-learning.html#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="supervised-learning.html#cb55-7" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb55-8"><a href="supervised-learning.html#cb55-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> iris.data, iris.target</span>
<span id="cb55-9"><a href="supervised-learning.html#cb55-9" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> tree.DecisionTreeClassifier()</span>
<span id="cb55-10"><a href="supervised-learning.html#cb55-10" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> clf.fit(X, y)</span>
<span id="cb55-11"><a href="supervised-learning.html#cb55-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-12"><a href="supervised-learning.html#cb55-12" aria-hidden="true" tabindex="-1"></a>tree.plot_tree(clf)</span></code></pre></div>
<p><img src="advanced_data_analysis_hco_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Unfortunately “the scikit-learn implementation does not support categorical variables for now” <span class="citation">(<a href="#ref-scikitDT" role="doc-biblioref"><span>“Decision Trees,”</span> n.d.</a>)</span>. Luckily the are <em>R</em> packages that allow the construction of decision trees based on categorical data. An example would be the <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf"><code>rpart</code> package</a>.</p>
</div>
<div id="eg-dt-r" class="section level3 hasAnchor" number="4.1.6">
<h3><span class="header-section-number">4.1.6</span> An Example of Creating a Decision Tree with <em>R</em><a href="supervised-learning.html#eg-dt-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below you see an example of a decision tree, which I created using the <code>rpart</code> package.</p>
<p>The algorithm used for the creation of the following decision tree is based on the <strong>CART</strong> algorithm <span class="citation">(<a href="#ref-Breiman1984" role="doc-biblioref">Breiman, 1984</a>)</span>.</p>
<div id="prepro-dt-r" class="section level4 hasAnchor" number="4.1.6.1">
<h4><span class="header-section-number">4.1.6.1</span> Preprocessing of Data<a href="supervised-learning.html#prepro-dt-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="supervised-learning.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">678</span>)</span>
<span id="cb56-2"><a href="supervised-learning.html#cb56-2" aria-hidden="true" tabindex="-1"></a>path <span class="ot">&lt;-</span> <span class="st">&#39;https://raw.githubusercontent.com/guru99-edu/R-Programming/master/titanic_data.csv&#39;</span></span>
<span id="cb56-3"><a href="supervised-learning.html#cb56-3" aria-hidden="true" tabindex="-1"></a>titanic <span class="ot">&lt;-</span><span class="fu">read.csv</span>(path)</span>
<span id="cb56-4"><a href="supervised-learning.html#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="supervised-learning.html#cb56-5" aria-hidden="true" tabindex="-1"></a>shuffle_index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(titanic))</span>
<span id="cb56-6"><a href="supervised-learning.html#cb56-6" aria-hidden="true" tabindex="-1"></a>titanic <span class="ot">&lt;-</span> titanic[shuffle_index, ]</span>
<span id="cb56-7"><a href="supervised-learning.html#cb56-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-8"><a href="supervised-learning.html#cb56-8" aria-hidden="true" tabindex="-1"></a>preclean_titanic <span class="ot">&lt;-</span> titanic</span>
<span id="cb56-9"><a href="supervised-learning.html#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="supervised-learning.html#cb56-10" aria-hidden="true" tabindex="-1"></a>preclean_titanic<span class="sc">$</span>age <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(preclean_titanic<span class="sc">$</span>age)</span></code></pre></div>
<pre><code>## Warning: NAs durch Umwandlung erzeugt</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="supervised-learning.html#cb58-1" aria-hidden="true" tabindex="-1"></a>preclean_titanic<span class="sc">$</span>fare <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(preclean_titanic<span class="sc">$</span>fare)</span></code></pre></div>
<pre><code>## Warning: NAs durch Umwandlung erzeugt</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="supervised-learning.html#cb60-1" aria-hidden="true" tabindex="-1"></a>clean_titanic <span class="ot">&lt;-</span> preclean_titanic <span class="sc">%&gt;%</span></span>
<span id="cb60-2"><a href="supervised-learning.html#cb60-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(home.dest, cabin, name, x, ticket)) <span class="sc">%&gt;%</span> <span class="co"># Drop variables</span></span>
<span id="cb60-3"><a href="supervised-learning.html#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pclass =</span> <span class="fu">factor</span>(pclass, <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="co">#Convert to factor level</span></span>
<span id="cb60-4"><a href="supervised-learning.html#cb60-4" aria-hidden="true" tabindex="-1"></a>                           <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&#39;Upper&#39;</span>, <span class="st">&#39;Middle&#39;</span>, <span class="st">&#39;Lower&#39;</span>)),</span>
<span id="cb60-5"><a href="supervised-learning.html#cb60-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">survived =</span> <span class="fu">factor</span>(survived, <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="co">#Convert to factor level</span></span>
<span id="cb60-6"><a href="supervised-learning.html#cb60-6" aria-hidden="true" tabindex="-1"></a>                           <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&#39;No&#39;</span>, <span class="st">&#39;Yes&#39;</span>)),</span>
<span id="cb60-7"><a href="supervised-learning.html#cb60-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">sex =</span> <span class="fu">factor</span>(sex),</span>
<span id="cb60-8"><a href="supervised-learning.html#cb60-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">embarked =</span> <span class="fu">factor</span>(embarked)) <span class="sc">%&gt;%</span></span>
<span id="cb60-9"><a href="supervised-learning.html#cb60-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span></code></pre></div>
</div>
<div id="traintest-dt-r" class="section level4 hasAnchor" number="4.1.6.2">
<h4><span class="header-section-number">4.1.6.2</span> Creating a Train and Test Dataset<a href="supervised-learning.html#traintest-dt-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="supervised-learning.html#cb61-1" aria-hidden="true" tabindex="-1"></a>create_train_test <span class="ot">&lt;-</span> <span class="cf">function</span>(data, <span class="at">size =</span> <span class="fl">0.8</span>, <span class="at">train =</span> <span class="cn">TRUE</span>) {</span>
<span id="cb61-2"><a href="supervised-learning.html#cb61-2" aria-hidden="true" tabindex="-1"></a>    n_row <span class="ot">=</span> <span class="fu">nrow</span>(data)</span>
<span id="cb61-3"><a href="supervised-learning.html#cb61-3" aria-hidden="true" tabindex="-1"></a>    total_row <span class="ot">=</span> size <span class="sc">*</span> n_row</span>
<span id="cb61-4"><a href="supervised-learning.html#cb61-4" aria-hidden="true" tabindex="-1"></a>    train_sample <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>total_row</span>
<span id="cb61-5"><a href="supervised-learning.html#cb61-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (train <span class="sc">==</span> <span class="cn">TRUE</span>) {</span>
<span id="cb61-6"><a href="supervised-learning.html#cb61-6" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(data[train_sample, ])</span>
<span id="cb61-7"><a href="supervised-learning.html#cb61-7" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb61-8"><a href="supervised-learning.html#cb61-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(data[<span class="sc">-</span>train_sample, ])</span>
<span id="cb61-9"><a href="supervised-learning.html#cb61-9" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb61-10"><a href="supervised-learning.html#cb61-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb61-11"><a href="supervised-learning.html#cb61-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-12"><a href="supervised-learning.html#cb61-12" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> <span class="fu">create_train_test</span>(clean_titanic, <span class="fl">0.8</span>, <span class="at">train =</span> <span class="cn">TRUE</span>)  <span class="co"># Train dataset with 80% of original data</span></span>
<span id="cb61-13"><a href="supervised-learning.html#cb61-13" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> <span class="fu">create_train_test</span>(clean_titanic, <span class="fl">0.8</span>, <span class="at">train =</span> <span class="cn">FALSE</span>)  <span class="co"># test dataset with 20% of original data</span></span></code></pre></div>
</div>
<div id="crevi-dt-r" class="section level4 hasAnchor" number="4.1.6.3">
<h4><span class="header-section-number">4.1.6.3</span> Creating and Visualizing the Decision Tree<a href="supervised-learning.html#crevi-dt-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="supervised-learning.html#cb62-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(survived <span class="sc">~</span> ., <span class="at">data =</span> data_train, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb62-2"><a href="supervised-learning.html#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="supervised-learning.html#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(fit, <span class="at">extra =</span> <span class="dv">106</span>)</span></code></pre></div>
<p><img src="advanced_data_analysis_hco_files/figure-html/rpart-crevi-1.png" width="672" /></p>
</div>
<div id="predict-dt-r" class="section level4 hasAnchor" number="4.1.6.4">
<h4><span class="header-section-number">4.1.6.4</span> Prediction of Data<a href="supervised-learning.html#predict-dt-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="supervised-learning.html#cb63-1" aria-hidden="true" tabindex="-1"></a>predict_unseen <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, data_test, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb63-2"><a href="supervised-learning.html#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="supervised-learning.html#cb63-3" aria-hidden="true" tabindex="-1"></a>table_mat <span class="ot">&lt;-</span> <span class="fu">table</span>(data_test<span class="sc">$</span>survived, predict_unseen)</span>
<span id="cb63-4"><a href="supervised-learning.html#cb63-4" aria-hidden="true" tabindex="-1"></a>table_mat</span></code></pre></div>
<pre><code>##      predict_unseen
##        No Yes
##   No  105  11
##   Yes  34  59</code></pre>
</div>
</div>
<div id="adv-disadv-dt" class="section level3 hasAnchor" number="4.1.7">
<h3><span class="header-section-number">4.1.7</span> Advantages and Disadvantages of Decision Trees<a href="supervised-learning.html#adv-disadv-dt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the big advantages of decision trees is, that they are incredibly <em>simple to understand</em>, <em>to interpret</em> and <em>to visualize</em>. Furthermore they are generally able to handle both <em>numerical</em> and <em>categorical data</em>. Another advantage is, that they can not only solve classification tasks, but regression tasks as well. Besides that they can even handle multi-output problems (problems where several outputs need to be predicted).</p>
<p>Unfortunately, like any other algorithms or statistical methods, decision tree algorithms do have several disadvantages as well. The main and most important disadvantage of decision trees though is the problem of <em>overfitting</em>. As I have explained before decision tree algorithms are non-compensatory algorithms, i.e. they ignore data (see <a href="supervised-learning.html#what-dt">above</a>). This does not mean though, that they are always simple. Quite the opposite in fact. Without the appropriate necessary restrictions decision trees can become highly complex networks of questions containing dozens or - depending on the respective dataset - even hundreds of dozens of nodes. Although such complex decision trees usually describe the data, which they were trained with, very well, they tend to be exceptionally bad at predicting data.</p>
<p>Fortunately the problem of overfitting can be overcome by carefully pruning - i.e. trimming off certain branches of the decision tree - without decreasing the overall accuracy of the decision tree algorithm. One algorithm used to achieve this is the <em>minimal cost-complexity pruning</em> algorithm.</p>
<p><em>For further information on the advantages and disadvantages of decision trees please read the respective <a href="https://scikit-learn.org/stable/modules/tree.html">article</a> on the scikit-learn website</em> <span class="citation">(<a href="#ref-scikitDT" role="doc-biblioref"><span>“Decision Trees,”</span> n.d.</a>)</span><em>.</em></p>
</div>
<div id="fft" class="section level3 hasAnchor" number="4.1.8">
<h3><span class="header-section-number">4.1.8</span> Fast-and-Frugal Trees<a href="supervised-learning.html#fft" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another solution for the problem of overfitting is the usage of more restrictive forms of decision tree algorithms. One of the most restrictive forms of a decision tree is a <em>fast-and-frugal tree</em> <span class="citation">(<a href="#ref-Neth2017" role="doc-biblioref">Phillips et al., 2017</a>)</span>.</p>
<p>Based on the research by Gigerenzer and colleagues on the topic of <em>heuristics</em> Phillips, Neth (University of Constance), Woike and Gaissmaier (University of Constance) <span class="citation">(<a href="#ref-Neth2017" role="doc-biblioref">2017</a>)</span> build the <em>R</em> package <code>FFTrees</code>, that allows users to easily create, visualize, and evaluate fast-and-frugal trees. Furthermore the package introduces a very handy new class of algorithms for constructing fast-and-frugal trees.</p>
</div>
<div id="fft-creation" class="section level3 hasAnchor" number="4.1.9">
<h3><span class="header-section-number">4.1.9</span> Creating a FFT with <code>FFTrees</code><a href="supervised-learning.html#fft-creation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="supervised-learning.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Create FFTs from training data and test on testing data heart.</span></span>
<span id="cb65-2"><a href="supervised-learning.html#cb65-2" aria-hidden="true" tabindex="-1"></a>heart.fft <span class="ot">&lt;-</span> <span class="fu">FFTrees</span>(<span class="at">formula =</span> diagnosis <span class="sc">~</span> ., <span class="co"># Criterion</span></span>
<span id="cb65-3"><a href="supervised-learning.html#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="at">data =</span> heart.train, <span class="co"># Training data </span></span>
<span id="cb65-4"><a href="supervised-learning.html#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="at">data.test =</span> heart.test, <span class="co"># Testing data</span></span>
<span id="cb65-5"><a href="supervised-learning.html#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="at">main =</span> <span class="st">&quot;Heart Disease&quot;</span>, <span class="co"># Optional labels </span></span>
<span id="cb65-6"><a href="supervised-learning.html#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="at">decision.labels =</span> <span class="fu">c</span>(<span class="st">&quot;Low-Risk&quot;</span>, <span class="st">&quot;High-Risk&quot;</span>))</span></code></pre></div>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="supervised-learning.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Create FFTs from training data and test on testing data heart.</span></span>
<span id="cb66-2"><a href="supervised-learning.html#cb66-2" aria-hidden="true" tabindex="-1"></a>heart.fft <span class="ot">&lt;-</span> <span class="fu">FFTrees</span>(<span class="at">formula =</span> diagnosis <span class="sc">~</span> ., <span class="co"># Criterion</span></span>
<span id="cb66-3"><a href="supervised-learning.html#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="at">data =</span> heart.train, <span class="co"># Training data </span></span>
<span id="cb66-4"><a href="supervised-learning.html#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="at">data.test =</span> heart.test, <span class="co"># Testing data</span></span>
<span id="cb66-5"><a href="supervised-learning.html#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="at">main =</span> <span class="st">&quot;Heart Disease&quot;</span>, <span class="co"># Optional labels </span></span>
<span id="cb66-6"><a href="supervised-learning.html#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="at">decision.labels =</span> <span class="fu">c</span>(<span class="st">&quot;Low-Risk&quot;</span>, <span class="st">&quot;High-Risk&quot;</span>))</span></code></pre></div>
<pre><code>## Setting &#39;goal = bacc&#39;</code></pre>
<pre><code>## Setting &#39;goal.chase = bacc&#39;</code></pre>
<pre><code>## Setting &#39;goal.threshold = bacc&#39;</code></pre>
<pre><code>## Setting cost.outcomes = list(hi = 0, mi = 1, fa = 1, cr = 0)</code></pre>
<pre><code>## Growing FFTs with ifan:</code></pre>
<pre><code>## 
[========================================&gt;------------------------------------]  54%
[==============================================&gt;------------------------------]  62%
[====================================================&gt;------------------------]  69%
[==========================================================&gt;------------------]  77%
[================================================================&gt;------------]  85%
[======================================================================&gt;------]  92%
[=============================================================================] 100%
## Fitting other algorithms for comparison (disable with do.comp = FALSE) ...</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="supervised-learning.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Inspect and summarize FFTs</span></span>
<span id="cb73-2"><a href="supervised-learning.html#cb73-2" aria-hidden="true" tabindex="-1"></a>heart.fft <span class="co"># Print statistics of the final FFT</span></span></code></pre></div>
<pre><code>## Heart Disease
## FFTrees 
## - Trees: 7 fast-and-frugal trees predicting diagnosis
## - Outcome costs: [hi = 0, mi = 1, fa = 1, cr = 0]
## 
## FFT #1: Definition
## [1] If thal = {rd,fd}, decide High-Risk.
## [2] If cp != {a}, decide Low-Risk.
## [3] If ca &gt; 0, decide High-Risk, otherwise, decide Low-Risk.
## 
## FFT #1: Training Accuracy
## Training data: N = 150, Pos (+) = 66 (44%) 
## 
## |          | True + | True - | Totals:
## |----------|--------|--------|
## | Decide + | hi  54 | fa  18 |      72
## | Decide - | mi  12 | cr  66 |      78
## |----------|--------|--------|
##   Totals:        66       84   N = 150
## 
## acc  = 80.0%   ppv  = 75.0%   npv  = 84.6%
## bacc = 80.2%   sens = 81.8%   spec = 78.6%
## 
## FFT #1: Training Speed, Frugality, and Cost
## mcu = 1.74,  pci = 0.87,  E(cost) = 0.200</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="supervised-learning.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">inwords</span>(heart.fft) <span class="co"># Print a verbal description of the final FFT</span></span></code></pre></div>
<pre><code>## [1] &quot;If thal = {rd,fd}, decide High-Risk.&quot;                    
## [2] &quot;If cp != {a}, decide Low-Risk.&quot;                          
## [3] &quot;If ca &gt; 0, decide High-Risk, otherwise, decide Low-Risk.&quot;</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="supervised-learning.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(heart.fft) <span class="co"># Print statistics of all FFTs</span></span></code></pre></div>
<pre><code>## Heart Disease
## 
## FFTrees 
## - Trees: 7 fast-and-frugal trees predicting diagnosis
## - Parameters: algorithm = &#39;ifan&#39;, goal = &#39;bacc&#39;, goal.chase = &#39;bacc&#39;,
##               sens.w = 0.5, max.levels = 4
## 
## 
## Table: (\#tab:fft)Tree definitions
## 
## | tree| nodes|classes |cues             |directions |thresholds          |exits     |
## |----:|-----:|:-------|:----------------|:----------|:-------------------|:---------|
## |    1|     3|c;c;n   |thal;cp;ca       |=;=;&gt;      |rd,fd;a;0           |1;0;0.5   |
## |    2|     4|c;c;n;c |thal;cp;ca;slope |=;=;&gt;;=    |rd,fd;a;0;flat,down |1;0;1;0.5 |
## |    3|     3|c;c;n   |thal;cp;ca       |=;=;&gt;      |rd,fd;a;0           |0;1;0.5   |
## |    4|     4|c;c;n;c |thal;cp;ca;slope |=;=;&gt;;=    |rd,fd;a;0;flat,down |1;1;0;0.5 |
## |    5|     3|c;c;n   |thal;cp;ca       |=;=;&gt;      |rd,fd;a;0           |0;0;0.5   |
## |    6|     4|c;c;n;c |thal;cp;ca;slope |=;=;&gt;;=    |rd,fd;a;0;flat,down |0;0;0;0.5 |
## |    7|     4|c;c;n;c |thal;cp;ca;slope |=;=;&gt;;=    |rd,fd;a;0;flat,down |1;1;1;0.5 |
## 
## 
## Table: (\#tab:fft)Tree statistics on training data
## 
## | tree|   n| hi| fa| mi| cr| sens| spec|  far|  ppv|  npv|  acc| bacc| wacc| cost_decisions| cost_cues| cost|  pci|  mcu|
## |----:|---:|--:|--:|--:|--:|----:|----:|----:|----:|----:|----:|----:|----:|--------------:|---------:|----:|----:|----:|
## |    1| 150| 54| 18| 12| 66| 0.82| 0.79| 0.21| 0.75| 0.85| 0.80| 0.80| 0.80|           0.20|         0| 0.20| 0.87| 1.74|
## |    2| 150| 57| 22|  9| 62| 0.86| 0.74| 0.26| 0.72| 0.87| 0.79| 0.80| 0.80|           0.21|         0| 0.21| 0.86| 1.84|
## |    3| 150| 44|  7| 22| 77| 0.67| 0.92| 0.08| 0.86| 0.78| 0.81| 0.79| 0.79|           0.19|         0| 0.19| 0.88| 1.56|
## |    4| 150| 60| 31|  6| 53| 0.91| 0.63| 0.37| 0.66| 0.90| 0.75| 0.77| 0.77|           0.25|         0| 0.25| 0.84| 2.12|
## |    5| 150| 28|  2| 38| 82| 0.42| 0.98| 0.02| 0.93| 0.68| 0.73| 0.70| 0.70|           0.27|         0| 0.27| 0.87| 1.70|
## |    6| 150| 21|  1| 45| 83| 0.32| 0.99| 0.01| 0.95| 0.65| 0.69| 0.65| 0.65|           0.31|         0| 0.31| 0.85| 1.90|
## |    7| 150| 64| 56|  2| 28| 0.97| 0.33| 0.67| 0.53| 0.93| 0.61| 0.65| 0.65|           0.39|         0| 0.39| 0.82| 2.30|
## 
## 
## Table: (\#tab:fft)Tree statistics on test data
## 
## | tree|   n| hi| fa| mi| cr| sens| spec|  far|  ppv|  npv|  acc| bacc| wacc| cost_decisions| cost_cues| cost|  pci|  mcu|
## |----:|---:|--:|--:|--:|--:|----:|----:|----:|----:|----:|----:|----:|----:|--------------:|---------:|----:|----:|----:|
## |    1| 153| 64| 19|  9| 61| 0.88| 0.76| 0.24| 0.77| 0.87| 0.82| 0.82| 0.82|           0.18|         0| 0.18| 0.87| 1.73|
## |    2| 153| 67| 26|  6| 54| 0.92| 0.68| 0.32| 0.72| 0.90| 0.79| 0.80| 0.80|           0.21|         0| 0.21| 0.86| 1.85|
## |    3| 153| 49|  8| 24| 72| 0.67| 0.90| 0.10| 0.86| 0.75| 0.79| 0.79| 0.79|           0.21|         0| 0.21| 0.87| 1.63|
## |    4| 153| 69| 36|  4| 44| 0.95| 0.55| 0.45| 0.66| 0.92| 0.74| 0.75| 0.75|           0.26|         0| 0.26| 0.85| 1.95|
## |    5| 153| 28|  0| 45| 80| 0.38| 1.00| 0.00| 1.00| 0.64| 0.71| 0.69| 0.69|           0.29|         0| 0.29| 0.86| 1.78|
## |    6| 153| 22|  0| 51| 80| 0.30| 1.00| 0.00| 1.00| 0.61| 0.67| 0.65| 0.65|           0.33|         0| 0.33| 0.85| 1.97|
## |    7| 153| 72| 56|  1| 24| 0.99| 0.30| 0.70| 0.56| 0.96| 0.63| 0.64| 0.64|           0.37|         0| 0.37| 0.84| 2.11|</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="supervised-learning.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Visualize the final FFT and performance results </span></span>
<span id="cb79-2"><a href="supervised-learning.html#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a) plot final FFT applied to test data:</span></span>
<span id="cb79-3"><a href="supervised-learning.html#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(heart.fft, <span class="at">data =</span> <span class="st">&quot;test&quot;</span>)</span></code></pre></div>
<p><img src="advanced_data_analysis_hco_files/figure-html/fft-1.png" width="672" /></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="supervised-learning.html#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># b) plot individual cue accuracies in ROC space:</span></span>
<span id="cb80-2"><a href="supervised-learning.html#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(heart.fft, <span class="at">what =</span> <span class="st">&quot;cues&quot;</span>)</span></code></pre></div>
<pre><code>## Using cue training statistics of object x:
## Cue accuracies ranked by bacc</code></pre>
<p><img src="advanced_data_analysis_hco_files/figure-html/fft-2.png" width="672" /></p>
</div>
<div id="dt-conclusion" class="section level3 hasAnchor" number="4.1.10">
<h3><span class="header-section-number">4.1.10</span> Conclusion<a href="supervised-learning.html#dt-conclusion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Decision tree algorithms are incredibly versatile and can be used in a number of contexts for wide range of problems. Due to their simplicity they are easy to understand, to interpret and to visualize.</p>
<p>As well as any other algorithms they do have certain libations of course. Thanks to the work for example of scientists like Phillips et al. <span class="citation">(<a href="#ref-Neth2017" role="doc-biblioref">2017</a>)</span>, many of these limitations can be overcome though.</p>
<p>Fast-and-frugal trees especially have an amazing potential in a number of fields.</p>
<p>Thus I recommend everyone to familiarize themselves with the usage of decision trees and <em>to take a stroll though this algorithmic jungle</em>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Breiman1984" class="csl-entry">
Breiman, L. (1984). <em>Classification and regression trees</em>. Wadsworth Internat. Group.
</div>
<div id="ref-KDChauhan2022" class="csl-entry">
Chauhan, N. S. (2022). Decision tree algorithm, explained. In <em>KDnuggets</em>. KDnuggets. <a href="https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html">https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html</a>
</div>
<div id="ref-scikitDT" class="csl-entry">
Decision trees. (n.d.). In <em>scikit-learn</em>. scikit-learn. Retrieved November 30, 2022, from <a href="https://scikit-learn.org/stable/modules/tree.html">https://scikit-learn.org/stable/modules/tree.html</a>
</div>
<div id="ref-Hastie2009" class="csl-entry">
Hastie, T., Tibshirani, R., &amp; Friedman, J. H. (2009). <em>The elements of statistical learning: Data mining, inference, and prediction</em> (2. ed.). Springer.
</div>
<div id="ref-Hunt1966" class="csl-entry">
Hunt, E. B., Marin, J., &amp; Stone, P. J. (1966). <em>Experiments in induction</em>. Academic Pr.
</div>
<div id="ref-Neth2017" class="csl-entry">
Phillips, N. D., Neth, H., Woike, J. K., &amp; Gaissmaier, W. (2017). FFTrees: A toolbox to create, visualize, and evaluate fast-and-frugal decision trees. <em>Judgment and Decision Making</em>, <em>12</em>(4), 344–368.
</div>
<div id="ref-Quinlan1986" class="csl-entry">
Quinlan, J. R. (1986). Induction of decision trees. <em>Machine Learning</em>, <em>1</em>(1), 81–106. <a href="https://doi.org/10.1007/BF00116251">https://doi.org/10.1007/BF00116251</a>
</div>
<div id="ref-Quinlan2003" class="csl-entry">
Quinlan, J. R. (2003). <em>C4.5: Programs for machine learning</em> (5. [pr.]). Morgan Kaufmann.
</div>
<div id="ref-deVille2013" class="csl-entry">
Ville, B. de. (2013). <em>WIREs Computational Statistics</em>, <em>5</em>(6), 448–455. https://doi.org/<a href="https://doi.org/10.1002/wics.1278">https://doi.org/10.1002/wics.1278</a>
</div>
<div id="ref-ibm" class="csl-entry">
What is a decision tree. (n.d.). In <em>IBM</em>. IBM. Retrieved November 30, 2022, from <a href="https://www.ibm.com/topics/decision-trees">https://www.ibm.com/topics/decision-trees</a>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="unsupervised-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="works-cited.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
